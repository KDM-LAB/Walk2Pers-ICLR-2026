{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d29e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4bcc80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Load T5-Base Model\n",
    "# ---------------------\n",
    "summarizer_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9be291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded nid2body with 113762 items\n",
      "üßæ Sample NID: N10000\n",
      "üìù Headline: Only FIVE internationals allowed, count em, FIVE! So first off we should say, per our usual Atlanta United lineup predictions, this will be wrong. Why will it be wrong? Well, aside from the obvious, we still don't have a ton of data points from Frank de Boer in how he prefers to rotate his team for \n"
     ]
    }
   ],
   "source": [
    "# Load nid2body from pickle\n",
    "with open(\"nid2body.pkl\", \"rb\") as f:\n",
    "    nid2body = pickle.load(f)\n",
    "\n",
    "# Debug print\n",
    "print(f\"‚úÖ Loaded nid2body with {len(nid2body)} items\")\n",
    "sample_nid = list(nid2body.keys())[0]\n",
    "print(f\"üßæ Sample NID: {sample_nid}\\nüìù Headline: {nid2body[sample_nid][:300]}\")"
   ]
  },
 
   "source": [
    "# Load sid2sum from pickle\n",
    "with open(\"sid2sum.pkl\", \"rb\") as f:\n",
    "    sid2sum = pickle.load(f)\n",
    "\n",
    "# Debug print\n",
    "print(f\"‚úÖ Loaded sid2sum with {len(sid2sum)} items\")\n",
    "sample_sid = list(sid2sum.keys())[0]\n",
    "print(f\"üßæ Sample SID: {sample_sid}\\nüìù Summary: {sid2sum[sample_sid][:300]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e60dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Device and Precision Setup ===\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_dim = 768\n",
    "\n",
    "# === Utility Functions ===\n",
    "def get_embedding(key, table, dim):\n",
    "    if key not in table:\n",
    "        table[key] = torch.nn.Parameter(torch.randn(dim, dtype=torch.float32, device=device) * 0.01, requires_grad=True)\n",
    "    return table[key]\n",
    "\n",
    "\n",
    "# === Load Embeddings ===\n",
    "with open(\"summary_T5.pkl\", \"rb\") as f:\n",
    "    summary_embed = {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in pickle.load(f).items()}\n",
    "with open(\"newsbody_T5.pkl\", \"rb\") as f:\n",
    "    newsbody_embed = {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in pickle.load(f).items()}\n",
    "with open(\"headline_T5.pkl\", \"rb\") as f:\n",
    "    headline_embed = {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in pickle.load(f).items()}\n",
    "\n",
    "embed_tables = {\n",
    "    'summary': summary_embed,\n",
    "    'newsbody': newsbody_embed,\n",
    "    'headline': headline_embed\n",
    "}\n",
    "\n",
    "# === Load Dataset ===\n",
    "lookup_df = pd.read_csv(\"w2p_test_engage_list_updated.csv\").set_index('EdgeID')\n",
    "train_df = pd.read_csv(\"w2p_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d592b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UserID</th>\n",
       "      <th>EHist</th>\n",
       "      <th>EPos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NT1_1</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NT1_2</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NT1_3</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NT1_4</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NT1_5</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20598</th>\n",
       "      <td>20598</td>\n",
       "      <td>NT99_196</td>\n",
       "      <td>['E37551', 'E37552', 'E37553', 'E37554', 'E375...</td>\n",
       "      <td>E70568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20599</th>\n",
       "      <td>20599</td>\n",
       "      <td>NT99_197</td>\n",
       "      <td>['E37551', 'E37552', 'E37553', 'E37554', 'E375...</td>\n",
       "      <td>E70570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20600</th>\n",
       "      <td>20600</td>\n",
       "      <td>NT99_198</td>\n",
       "      <td>['E37551', 'E37552', 'E37553', 'E37554', 'E375...</td>\n",
       "      <td>E70572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20601</th>\n",
       "      <td>20601</td>\n",
       "      <td>NT99_199</td>\n",
       "      <td>['E37551', 'E37552', 'E37553', 'E37554', 'E375...</td>\n",
       "      <td>E70574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20602</th>\n",
       "      <td>20602</td>\n",
       "      <td>NT99_200</td>\n",
       "      <td>['E37551', 'E37552', 'E37553', 'E37554', 'E375...</td>\n",
       "      <td>E70576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20603 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    UserID  \\\n",
       "0               0     NT1_1   \n",
       "1               1     NT1_2   \n",
       "2               2     NT1_3   \n",
       "3               3     NT1_4   \n",
       "4               4     NT1_5   \n",
       "...           ...       ...   \n",
       "20598       20598  NT99_196   \n",
       "20599       20599  NT99_197   \n",
       "20600       20600  NT99_198   \n",
       "20601       20601  NT99_199   \n",
       "20602       20602  NT99_200   \n",
       "\n",
       "                                                   EHist    EPos  \n",
       "0      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...    E274  \n",
       "1      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...    E276  \n",
       "2      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...    E278  \n",
       "3      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...    E280  \n",
       "4      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...    E282  \n",
       "...                                                  ...     ...  \n",
       "20598  ['E37551', 'E37552', 'E37553', 'E37554', 'E375...  E70568  \n",
       "20599  ['E37551', 'E37552', 'E37553', 'E37554', 'E375...  E70570  \n",
       "20600  ['E37551', 'E37552', 'E37553', 'E37554', 'E375...  E70572  \n",
       "20601  ['E37551', 'E37552', 'E37553', 'E37554', 'E375...  E70574  \n",
       "20602  ['E37551', 'E37552', 'E37553', 'E37554', 'E375...  E70576  \n",
       "\n",
       "[20603 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88fbd17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 UserID                                              EHist  EPos\n",
      "0           0  NT1_1  [E124, E125, E126, E127, E128, E129, E130, E13...  E274\n",
      "1           1  NT1_2  [E126, E127, E128, E129, E130, E131, E132, E13...  E276\n",
      "2           2  NT1_3  [E128, E129, E130, E131, E132, E133, E134, E13...  E278\n",
      "3           3  NT1_4  [E130, E131, E132, E133, E134, E135, E136, E13...  E280\n",
      "4           4  NT1_5  [E132, E133, E134, E135, E136, E137, E138, E13...  E282\n",
      "count    20603.0\n",
      "mean       150.0\n",
      "std          0.0\n",
      "min        150.0\n",
      "25%        150.0\n",
      "50%        150.0\n",
      "75%        150.0\n",
      "max        150.0\n",
      "Name: EHist, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Ensure EHist column is parsed into lists (currently it's a string of list)\n",
    "train_df[\"EHist\"] = train_df[\"EHist\"].apply(ast.literal_eval)\n",
    "\n",
    "# Keep only last 150 elements\n",
    "train_df[\"EHist\"] = train_df[\"EHist\"].apply(lambda x: x[-150:])\n",
    "\n",
    "# Verify\n",
    "print(train_df.head())\n",
    "print(train_df[\"EHist\"].apply(len).describe())  # check max length = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "915bd782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UserID</th>\n",
       "      <th>EHist</th>\n",
       "      <th>EPos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NT1_1</td>\n",
       "      <td>[E124, E125, E126, E127, E128, E129, E130, E13...</td>\n",
       "      <td>E274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NT1_2</td>\n",
       "      <td>[E126, E127, E128, E129, E130, E131, E132, E13...</td>\n",
       "      <td>E276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NT1_3</td>\n",
       "      <td>[E128, E129, E130, E131, E132, E133, E134, E13...</td>\n",
       "      <td>E278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NT1_4</td>\n",
       "      <td>[E130, E131, E132, E133, E134, E135, E136, E13...</td>\n",
       "      <td>E280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NT1_5</td>\n",
       "      <td>[E132, E133, E134, E135, E136, E137, E138, E13...</td>\n",
       "      <td>E282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20598</th>\n",
       "      <td>20598</td>\n",
       "      <td>NT99_196</td>\n",
       "      <td>[E70418, E70419, E70420, E70421, E70422, E7042...</td>\n",
       "      <td>E70568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20599</th>\n",
       "      <td>20599</td>\n",
       "      <td>NT99_197</td>\n",
       "      <td>[E70420, E70421, E70422, E70423, E70424, E7042...</td>\n",
       "      <td>E70570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20600</th>\n",
       "      <td>20600</td>\n",
       "      <td>NT99_198</td>\n",
       "      <td>[E70422, E70423, E70424, E70425, E70426, E7042...</td>\n",
       "      <td>E70572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20601</th>\n",
       "      <td>20601</td>\n",
       "      <td>NT99_199</td>\n",
       "      <td>[E70424, E70425, E70426, E70427, E70428, E7042...</td>\n",
       "      <td>E70574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20602</th>\n",
       "      <td>20602</td>\n",
       "      <td>NT99_200</td>\n",
       "      <td>[E70426, E70427, E70428, E70429, E70430, E7043...</td>\n",
       "      <td>E70576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20603 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    UserID  \\\n",
       "0               0     NT1_1   \n",
       "1               1     NT1_2   \n",
       "2               2     NT1_3   \n",
       "3               3     NT1_4   \n",
       "4               4     NT1_5   \n",
       "...           ...       ...   \n",
       "20598       20598  NT99_196   \n",
       "20599       20599  NT99_197   \n",
       "20600       20600  NT99_198   \n",
       "20601       20601  NT99_199   \n",
       "20602       20602  NT99_200   \n",
       "\n",
       "                                                   EHist    EPos  \n",
       "0      [E124, E125, E126, E127, E128, E129, E130, E13...    E274  \n",
       "1      [E126, E127, E128, E129, E130, E131, E132, E13...    E276  \n",
       "2      [E128, E129, E130, E131, E132, E133, E134, E13...    E278  \n",
       "3      [E130, E131, E132, E133, E134, E135, E136, E13...    E280  \n",
       "4      [E132, E133, E134, E135, E136, E137, E138, E13...    E282  \n",
       "...                                                  ...     ...  \n",
       "20598  [E70418, E70419, E70420, E70421, E70422, E7042...  E70568  \n",
       "20599  [E70420, E70421, E70422, E70423, E70424, E7042...  E70570  \n",
       "20600  [E70422, E70423, E70424, E70425, E70426, E7042...  E70572  \n",
       "20601  [E70424, E70425, E70426, E70427, E70428, E7042...  E70574  \n",
       "20602  [E70426, E70427, E70428, E70429, E70430, E7043...  E70576  \n",
       "\n",
       "[20603 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5427da34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [E124, E125, E126, E127, E128, E129, E130, E13...\n",
      "1    [E126, E127, E128, E129, E130, E131, E132, E13...\n",
      "2    [E128, E129, E130, E131, E132, E133, E134, E13...\n",
      "3    [E130, E131, E132, E133, E134, E135, E136, E13...\n",
      "4    [E132, E133, E134, E135, E136, E137, E138, E13...\n",
      "Name: EHist, dtype: object\n",
      "Max length + 1: 151\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()  # enable progress_apply\n",
    "\n",
    "# Convert all Bhist strings to actual lists with progress bar\n",
    "# train_df['EHist'] = train_df['EHist'].progress_apply(ast_eval)\n",
    "\n",
    "# Check a few entries\n",
    "print(train_df['EHist'].head())\n",
    "\n",
    "# Compute max length\n",
    "max_len = max(len(h) for h in train_df[\"EHist\"])\n",
    "max_len_plus_one = max_len + 1\n",
    "print(\"Max length + 1:\", max_len_plus_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3241ec70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UserID</th>\n",
       "      <th>EHist</th>\n",
       "      <th>EPos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NT1_1</td>\n",
       "      <td>[E124, E125, E126, E127, E128, E129, E130, E13...</td>\n",
       "      <td>E274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NT1_2</td>\n",
       "      <td>[E126, E127, E128, E129, E130, E131, E132, E13...</td>\n",
       "      <td>E276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NT1_3</td>\n",
       "      <td>[E128, E129, E130, E131, E132, E133, E134, E13...</td>\n",
       "      <td>E278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NT1_4</td>\n",
       "      <td>[E130, E131, E132, E133, E134, E135, E136, E13...</td>\n",
       "      <td>E280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NT1_5</td>\n",
       "      <td>[E132, E133, E134, E135, E136, E137, E138, E13...</td>\n",
       "      <td>E282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20598</th>\n",
       "      <td>20598</td>\n",
       "      <td>NT99_196</td>\n",
       "      <td>[E70418, E70419, E70420, E70421, E70422, E7042...</td>\n",
       "      <td>E70568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20599</th>\n",
       "      <td>20599</td>\n",
       "      <td>NT99_197</td>\n",
       "      <td>[E70420, E70421, E70422, E70423, E70424, E7042...</td>\n",
       "      <td>E70570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20600</th>\n",
       "      <td>20600</td>\n",
       "      <td>NT99_198</td>\n",
       "      <td>[E70422, E70423, E70424, E70425, E70426, E7042...</td>\n",
       "      <td>E70572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20601</th>\n",
       "      <td>20601</td>\n",
       "      <td>NT99_199</td>\n",
       "      <td>[E70424, E70425, E70426, E70427, E70428, E7042...</td>\n",
       "      <td>E70574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20602</th>\n",
       "      <td>20602</td>\n",
       "      <td>NT99_200</td>\n",
       "      <td>[E70426, E70427, E70428, E70429, E70430, E7043...</td>\n",
       "      <td>E70576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20603 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    UserID  \\\n",
       "0               0     NT1_1   \n",
       "1               1     NT1_2   \n",
       "2               2     NT1_3   \n",
       "3               3     NT1_4   \n",
       "4               4     NT1_5   \n",
       "...           ...       ...   \n",
       "20598       20598  NT99_196   \n",
       "20599       20599  NT99_197   \n",
       "20600       20600  NT99_198   \n",
       "20601       20601  NT99_199   \n",
       "20602       20602  NT99_200   \n",
       "\n",
       "                                                   EHist    EPos  \n",
       "0      [E124, E125, E126, E127, E128, E129, E130, E13...    E274  \n",
       "1      [E126, E127, E128, E129, E130, E131, E132, E13...    E276  \n",
       "2      [E128, E129, E130, E131, E132, E133, E134, E13...    E278  \n",
       "3      [E130, E131, E132, E133, E134, E135, E136, E13...    E280  \n",
       "4      [E132, E133, E134, E135, E136, E137, E138, E13...    E282  \n",
       "...                                                  ...     ...  \n",
       "20598  [E70418, E70419, E70420, E70421, E70422, E7042...  E70568  \n",
       "20599  [E70420, E70421, E70422, E70423, E70424, E7042...  E70570  \n",
       "20600  [E70422, E70423, E70424, E70425, E70426, E7042...  E70572  \n",
       "20601  [E70424, E70425, E70426, E70427, E70428, E7042...  E70574  \n",
       "20602  [E70426, E70427, E70428, E70429, E70430, E7043...  E70576  \n",
       "\n",
       "[20603 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "364646e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UserID</th>\n",
       "      <th>EHist</th>\n",
       "      <th>EPos</th>\n",
       "      <th>Bhist_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NT1_1</td>\n",
       "      <td>[E124, E125, E126, E127, E128, E129, E130, E13...</td>\n",
       "      <td>E274</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NT1_2</td>\n",
       "      <td>[E126, E127, E128, E129, E130, E131, E132, E13...</td>\n",
       "      <td>E276</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NT1_3</td>\n",
       "      <td>[E128, E129, E130, E131, E132, E133, E134, E13...</td>\n",
       "      <td>E278</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NT1_4</td>\n",
       "      <td>[E130, E131, E132, E133, E134, E135, E136, E13...</td>\n",
       "      <td>E280</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NT1_5</td>\n",
       "      <td>[E132, E133, E134, E135, E136, E137, E138, E13...</td>\n",
       "      <td>E282</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20598</th>\n",
       "      <td>20598</td>\n",
       "      <td>NT99_196</td>\n",
       "      <td>[E70418, E70419, E70420, E70421, E70422, E7042...</td>\n",
       "      <td>E70568</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20599</th>\n",
       "      <td>20599</td>\n",
       "      <td>NT99_197</td>\n",
       "      <td>[E70420, E70421, E70422, E70423, E70424, E7042...</td>\n",
       "      <td>E70570</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20600</th>\n",
       "      <td>20600</td>\n",
       "      <td>NT99_198</td>\n",
       "      <td>[E70422, E70423, E70424, E70425, E70426, E7042...</td>\n",
       "      <td>E70572</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20601</th>\n",
       "      <td>20601</td>\n",
       "      <td>NT99_199</td>\n",
       "      <td>[E70424, E70425, E70426, E70427, E70428, E7042...</td>\n",
       "      <td>E70574</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20602</th>\n",
       "      <td>20602</td>\n",
       "      <td>NT99_200</td>\n",
       "      <td>[E70426, E70427, E70428, E70429, E70430, E7043...</td>\n",
       "      <td>E70576</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20603 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    UserID  \\\n",
       "0               0     NT1_1   \n",
       "1               1     NT1_2   \n",
       "2               2     NT1_3   \n",
       "3               3     NT1_4   \n",
       "4               4     NT1_5   \n",
       "...           ...       ...   \n",
       "20598       20598  NT99_196   \n",
       "20599       20599  NT99_197   \n",
       "20600       20600  NT99_198   \n",
       "20601       20601  NT99_199   \n",
       "20602       20602  NT99_200   \n",
       "\n",
       "                                                   EHist    EPos  Bhist_len  \n",
       "0      [E124, E125, E126, E127, E128, E129, E130, E13...    E274        150  \n",
       "1      [E126, E127, E128, E129, E130, E131, E132, E13...    E276        150  \n",
       "2      [E128, E129, E130, E131, E132, E133, E134, E13...    E278        150  \n",
       "3      [E130, E131, E132, E133, E134, E135, E136, E13...    E280        150  \n",
       "4      [E132, E133, E134, E135, E136, E137, E138, E13...    E282        150  \n",
       "...                                                  ...     ...        ...  \n",
       "20598  [E70418, E70419, E70420, E70421, E70422, E7042...  E70568        150  \n",
       "20599  [E70420, E70421, E70422, E70423, E70424, E7042...  E70570        150  \n",
       "20600  [E70422, E70423, E70424, E70425, E70426, E7042...  E70572        150  \n",
       "20601  [E70424, E70425, E70426, E70427, E70428, E7042...  E70574        150  \n",
       "20602  [E70426, E70427, E70428, E70429, E70430, E7043...  E70576        150  \n",
       "\n",
       "[20603 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute lengths of each history\n",
    "train_df[\"Bhist_len\"] = train_df[\"EHist\"].apply(len)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5378608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# BehaviorEncoder (uses padded Bhist + Bhist_len)\n",
    "# ======================================================\n",
    "class BehaviorEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, device, max_len, debug=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.debug = debug\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # === Base action vectors ===\n",
    "        self.e_clk = nn.Parameter(torch.tensor([1., 0., 0., 0.], device=device))\n",
    "        self.e_skp = nn.Parameter(torch.tensor([0., 1., 0., 0.], device=device))\n",
    "        self.e_gensumm = nn.Parameter(torch.tensor([0., 0., 1., 0.], device=device))\n",
    "        self.e_sumgen = nn.Parameter(torch.tensor([0., 0., 0., 1.], device=device))\n",
    "\n",
    "        # Action-specific transforms\n",
    "        self.W_clk = nn.Linear(4, hidden_dim, bias=False)\n",
    "        self.W_skp = nn.Linear(4, hidden_dim, bias=False)\n",
    "        self.W_gensumm = nn.Linear(4, hidden_dim, bias=False)\n",
    "        self.W_sumgen = nn.Linear(4, hidden_dim, bias=False)\n",
    "\n",
    "        # State transforms\n",
    "        self.W_pull = nn.Linear(1, hidden_dim, bias=False)\n",
    "        self.W_s = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_d = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "        # Fusion\n",
    "        self.Wh = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wc = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wz = nn.Linear(hidden_dim, 3, bias=False)\n",
    "        self.b_z = nn.Parameter(torch.zeros(3, device=device))\n",
    "        self.W_emb = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.b_emb = nn.Parameter(torch.zeros(hidden_dim, device=device))  # kept as in your design\n",
    "\n",
    "        # Rotation/translation\n",
    "        self.W_angle = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_theta = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        self.W_h = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_m = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "        # Scalars\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5, device=device))\n",
    "        self.beta = nn.Parameter(torch.tensor(0.5, device=device))\n",
    "\n",
    "        # Classifier head over timesteps (0...max_len+1)\n",
    "        self.classifier = nn.Linear(hidden_dim, max_len + 2)\n",
    "\n",
    "        # Next-step prediction head\n",
    "        self.W_next = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def softmin_pool(self, a, b):\n",
    "        return -self.alpha * torch.log(torch.exp(a / self.alpha) +\n",
    "                                       torch.exp(b / self.alpha) + 1e-9)\n",
    "\n",
    "    def forward(self, Bhist, Bhist_len, Bpos, lookup_df, embed_tables):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          - Bhist: padded list/sequence of EIDs (length == self.max_len)\n",
    "          - Bhist_len: original unpadded length (int)\n",
    "          - Bpos: target EID (scalar id)\n",
    "          - lookup_df: DataFrame indexed by EID with columns 'Tail','Head','Relation'\n",
    "          - embed_tables: dict with keys 'newsbody','summary','headline' mapping item-id -> tensor(hidden_dim,)\n",
    "        Outputs:\n",
    "          - eprime_last: final contextualized embedding (for last real token)\n",
    "          - eprime_next: predicted next-step embedding (W_next(eprime_last))\n",
    "          - logits_pos: classifier logits for the next-step (vector of size max_len+2)\n",
    "          - ctx_enc_loss: accumulated stepwise classification loss (sum over real steps)\n",
    "          - total_loss: weighted total loss (0.2 * step-loss + 0.8 * next-pos CE) as you had\n",
    "        Notes:\n",
    "          - This function explicitly iterates only over the first Bhist_len entries (ignoring PADs).\n",
    "        \"\"\"\n",
    "        # ensure Bhist is indexable (list/1-D tensor). If tensor, bring to cpu/int list for lookup indexing\n",
    "        if isinstance(Bhist, torch.Tensor):\n",
    "            Bhist_list = Bhist.detach().cpu().tolist()\n",
    "        else:\n",
    "            Bhist_list = list(Bhist)\n",
    "\n",
    "        # initialize losses on device\n",
    "        ctx_enc_loss = torch.tensor(0., dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # PASS 1: build per-step raw embeddings E_seq for real tokens only\n",
    "        E_seq = []\n",
    "        h_clk = torch.zeros(self.hidden_dim, device=self.device)\n",
    "        h_skp = torch.zeros(self.hidden_dim, device=self.device)\n",
    "        h = torch.zeros(self.hidden_dim, device=self.device)\n",
    "\n",
    "        for t in range(int(Bhist_len)):  # iterate only real tokens\n",
    "            b_id = Bhist_list[t]\n",
    "            # skip if missing in lookup\n",
    "            if b_id not in lookup_df.index:\n",
    "                continue\n",
    "\n",
    "            row = lookup_df.loc[b_id]\n",
    "            tail_id, rel = row['Tail'], row['Relation']\n",
    "\n",
    "            # fetch embeddings (fallback zeros)\n",
    "            d_i = embed_tables['newsbody'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "            s_i = embed_tables['summary'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "            d_i_title = embed_tables['headline'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "\n",
    "            # initialize at t==0\n",
    "            if t == 0:\n",
    "                head_emb = embed_tables['headline'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "                h_clk, h_skp = head_emb.clone(), head_emb.clone()\n",
    "                # keep original math: W_s(head_emb) * h_clk + (1-W_s(head_emb)) * h_skp\n",
    "                h = torch.tanh(self.W_s(head_emb) * h_clk + (1 - (self.W_s(head_emb))) * h_skp)\n",
    "\n",
    "            # relation-specific context\n",
    "            if rel == \"click\":\n",
    "                c_i = torch.sigmoid(self.W_clk.weight @ self.e_clk * h * d_i)\n",
    "            elif rel == \"skip\":\n",
    "                d_ip1 = torch.zeros_like(d_i)\n",
    "                if (t + 1) < Bhist_len and Bhist_list[t+1] in lookup_df.index:\n",
    "                    next_row = lookup_df.loc[Bhist_list[t+1]]\n",
    "                    d_ip1 = embed_tables['newsbody'].get(next_row['Head'], torch.zeros_like(d_i))\n",
    "                pull_term = self.W_pull(torch.tensor([[torch.dot(h_clk, d_ip1) + torch.dot(h_skp, d_i)]],\n",
    "                                                     device=self.device)).squeeze(0)\n",
    "                c_i = -torch.tanh(self.W_skp.weight @ self.e_skp + d_i + pull_term * h * d_i)\n",
    "            elif rel == \"gen_summ\":\n",
    "                c_i = torch.tanh(self.W_gensumm.weight @ self.e_gensumm * h * d_i_title)\n",
    "            elif rel == \"summ_gen\":\n",
    "                gate_summgen = self.W_s(self.W_sumgen.weight @ self.e_sumgen)\n",
    "                c_i = torch.tanh(self.softmin_pool(gate_summgen * s_i, (1 - gate_summgen) * d_i))\n",
    "                h_clk = h_clk + self.W_d((torch.ones_like(d_i_title) - d_i_title) * s_i)\n",
    "            else:\n",
    "                c_i = d_i\n",
    "\n",
    "            # hidden update\n",
    "            z_i = torch.tanh(self.Wh(h) + self.Wc(c_i))\n",
    "            p_i = torch.softmax(self.Wz(z_i) + self.b_z, dim=-1)\n",
    "            m_i = p_i[0] * 0.1 + p_i[1] * 0.5 + p_i[2] * 0.9\n",
    "            if rel == \"click\":\n",
    "                h_clk = h_clk + m_i * c_i\n",
    "            elif rel == \"skip\":\n",
    "                h_skp = h_skp * (1 - m_i) + c_i\n",
    "            h = (self.beta * h_clk + (1 - self.beta) * h_skp)\n",
    "\n",
    "            e_i = torch.tanh(self.W_emb(c_i))\n",
    "            E_seq.append(e_i)\n",
    "\n",
    "        # if E_seq is empty (no valid history), return zeros to avoid crash\n",
    "        if len(E_seq) == 0:\n",
    "            zero_e = torch.zeros(self.hidden_dim, device=self.device)\n",
    "            eprime_last = zero_e\n",
    "            eprime_next = self.W_next(eprime_last)\n",
    "            logits_pos = self.classifier(eprime_next.unsqueeze(0))\n",
    "            ctx_enc_loss = torch.tensor(0., device=self.device)\n",
    "            total_loss = torch.tensor(0., device=self.device)\n",
    "            return eprime_last, eprime_next, logits_pos, ctx_enc_loss, total_loss\n",
    "\n",
    "        # PASS 2: contextualize E_seq -> Eprime_seq (only for real tokens)\n",
    "        Eprime_seq = []\n",
    "        eps = 1e-6\n",
    "        for i, e_i in enumerate(E_seq):\n",
    "            if i == 0:\n",
    "                e_prime = e_i\n",
    "            else:\n",
    "                e_prev, e_prime_prev = E_seq[i-1], Eprime_seq[-1]\n",
    "                theta_i = math.pi * torch.tanh(self.W_theta(torch.sigmoid(self.W_angle(e_prime_prev))))\n",
    "                m_i = F.softplus(self.W_m(self.W_h(e_prime_prev)))\n",
    "\n",
    "                # compute orthonormal direction and rotation (as in your original)\n",
    "                v_i = (e_i - e_prime_prev) / (e_i - e_prime_prev).norm(p=2).clamp(min=eps)\n",
    "                u_prev = e_prev / e_prime_prev.norm(p=2).clamp(min=eps)\n",
    "                o_i = (v_i - torch.dot(v_i, u_prev) * u_prev)\n",
    "                o_i = o_i / o_i.norm(p=2).clamp(min=eps)\n",
    "\n",
    "                e_prime = e_prime_prev + m_i * (torch.cos(theta_i) * u_prev + torch.sin(theta_i) * o_i).squeeze()\n",
    "                e_prime = torch.tanh(e_prime) + e_i\n",
    "\n",
    "            Eprime_seq.append(e_prime)\n",
    "\n",
    "            # per-step classification loss only for real steps\n",
    "            target_step = torch.tensor([i], device=self.device)\n",
    "            logits_step = self.classifier(e_prime.unsqueeze(0))\n",
    "            ctx_enc_loss = ctx_enc_loss + F.cross_entropy(logits_step, target_step)\n",
    "\n",
    "        # Final prediction on Bpos (next-step)\n",
    "        eprime_last = Eprime_seq[-1]\n",
    "        eprime_next = self.W_next(eprime_last)\n",
    "        logits_pos = self.classifier(eprime_next.unsqueeze(0))\n",
    "\n",
    "        # target for next-position is Bhist_len (0-based indexing consistent with your design)\n",
    "        target_pos = torch.tensor([int(Bhist_len)], device=self.device)\n",
    "        total_loss = 0.2 * (ctx_enc_loss / max(1, float(Bhist_len))) + 0.8 * F.cross_entropy(logits_pos, target_pos)\n",
    "\n",
    "        return eprime_last, eprime_next, logits_pos, ctx_enc_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5aa1ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# BehaviorInverseDecoderPredict\n",
    "# ======================================================\n",
    "class BehaviorInverseDecoderPredict(nn.Module):\n",
    "    \"\"\"\n",
    "    Inverse mapping that takes a single predicted e' (embedding for Bpos)\n",
    "    and the head/headline embedding for Bpos, and returns:\n",
    "      - c'_pos (approx pseudo-content)\n",
    "      - s_hat_pos (approx summary)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, device, debug=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.debug = debug\n",
    "\n",
    "        # Learnable pseudo-inverse / disentanglers\n",
    "        self.W_emb_pinv = nn.Linear(hidden_dim, hidden_dim, bias=False)   # approx W_emb^+\n",
    "        self.W1_pinv = nn.Linear(hidden_dim, hidden_dim, bias=False)      # remove head contribution\n",
    "        self.W2_pinv = nn.Linear(hidden_dim, hidden_dim, bias=False)      # map residual -> summary\n",
    "\n",
    "    def _show(self, name, tensor, maxlen=6):\n",
    "        if not self.debug:\n",
    "            return\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            if tensor.ndim == 0:\n",
    "                print(f\"{name}: {tensor.item():.6f}\")\n",
    "            else:\n",
    "                flat = tensor.detach().cpu().numpy().flatten()\n",
    "                vals = \", \".join(f\"{x:.6f}\" for x in flat[:maxlen])\n",
    "                if len(flat) > maxlen: vals += \", ...\"\n",
    "                print(f\"{name} (shape={tuple(tensor.shape)}): [{vals}]\")\n",
    "        else:\n",
    "            print(f\"{name}: {tensor}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def atanh_safe(x, eps=1e-6):\n",
    "        x = x.clamp(-1+eps, 1-eps)\n",
    "        return 0.5 * torch.log((1+x) / (1-x))\n",
    "\n",
    "    def forward(self, eprime_pos, b_emb, h_pos):\n",
    "        \"\"\"\n",
    "        eprime_pos: tensor (hidden_dim,) -- predicted e' embedding for Bpos (assumed already on device)\n",
    "        b_emb: encoder bias b_emb (tensor (hidden_dim,))\n",
    "        h_pos: head/headline embedding for Bpos (tensor (hidden_dim,))\n",
    "        Returns: c_prime_pos, s_hat_pos  (both normalized)\n",
    "        \"\"\"\n",
    "        # 1) invert embedding nonlinearity: atanh(e') - b_emb\n",
    "        x = self.atanh_safe(eprime_pos) - b_emb  # (hidden_dim,)\n",
    "        c_prime_pos = self.W_emb_pinv(x)         # approx c'_pos\n",
    "        c_prime_pos = F.normalize(c_prime_pos, p=2, dim=0)\n",
    "\n",
    "        # 2) subtract head contribution and map to summary\n",
    "        residual = c_prime_pos - self.W1_pinv(h_pos)\n",
    "        s_hat_pos = self.W2_pinv(residual)\n",
    "        s_hat_pos = F.normalize(s_hat_pos, p=2, dim=0)\n",
    "\n",
    "        return c_prime_pos, s_hat_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e250e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop-in robust PersonalizedT5Summarizer (replace your previous class with this)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "class PersonalizedT5Summarizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Robust, drop-in Personalized T5 wrapper.\n",
    "    - Conservative personalization injection\n",
    "    - Defensive handling of gold summaries (NaN, None, non-string)\n",
    "    - Safe device/dtype management for encoder outputs & generate()\n",
    "    - Returns keys used by the training loop: total_loss, enc_loss, gen_loss, lm_logits, pred_summary_tf / pred_summary_ar\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, t5_model, behavior_encoder, inverse_decoder, device=None, learnable_ctx=True, enc_loss_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.t5 = t5_model\n",
    "        self.behavior_encoder = behavior_encoder\n",
    "        self.inverse_decoder = inverse_decoder\n",
    "        self.device = device if device is not None else (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "        self.t5_hidden_dim = self.t5.config.d_model\n",
    "        self.enc_loss_weight = float(enc_loss_weight)\n",
    "\n",
    "        # projection layers for personalization injection\n",
    "        self.e_proj = nn.Linear(hidden_dim, self.t5_hidden_dim)\n",
    "        self.q_proj = nn.Linear(self.t5_hidden_dim, self.t5_hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, self.t5_hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, self.t5_hidden_dim)\n",
    "\n",
    "        # small initial ctx scale (softplus(-2) ~ 0.12) -> conservative personalization\n",
    "        if learnable_ctx:\n",
    "            self.ctx_scale_raw = nn.Parameter(torch.tensor(-2.0))\n",
    "        else:\n",
    "            self.register_buffer(\"ctx_scale_raw\", torch.tensor(-2.0), persistent=False)\n",
    "\n",
    "        # Freeze all T5 params initially; unfreeze cross-attention and lm_head\n",
    "        for p in self.t5.parameters():\n",
    "            p.requires_grad = False\n",
    "        for n, p in self.t5.named_parameters():\n",
    "            if (\"decoder.block\" in n or \"decoder.layers\" in n or \"decoder.layer\" in n) and (\n",
    "                \"EncDecAttention\" in n or \"encdec_attn\" in n or \"cross_attn\" in n or \"encoder_attn\" in n\n",
    "            ):\n",
    "                p.requires_grad = True\n",
    "        if hasattr(self.t5, \"lm_head\"):\n",
    "            for p in self.t5.lm_head.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    def _behavior_encode(self, Bhist, Bhist_len, Bpos, lookup_df, embed_tables):\n",
    "        \"\"\"\n",
    "        Calls behavior_encoder and inverse_decoder to get e_last, s_hat and encoder loss info.\n",
    "        Supports behavior_encoder returning 5 or 6 elements.\n",
    "        \"\"\"\n",
    "        enc_ret = self.behavior_encoder(Bhist, Bhist_len, Bpos, lookup_df, embed_tables)\n",
    "        # normalize return length handling\n",
    "        if len(enc_ret) == 5:\n",
    "            e_last, e_next, logits_pos, enc_loss, total_loss = enc_ret\n",
    "            per_step_logits = None\n",
    "        else:\n",
    "            e_last, e_next, logits_pos, enc_loss, total_loss, per_step_logits = enc_ret\n",
    "\n",
    "        # resolve head embedding\n",
    "        try:\n",
    "            head_id = lookup_df.loc[Bpos]['Head']\n",
    "        except Exception:\n",
    "            head_id = Bpos\n",
    "        head_emb = embed_tables['headline'].get(head_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "\n",
    "        b_emb = getattr(self.behavior_encoder, \"b_emb\", torch.zeros(self.hidden_dim, device=self.device))\n",
    "\n",
    "        # ensure tensors on correct device\n",
    "        e_next = e_next.to(self.device) if isinstance(e_next, torch.Tensor) else torch.tensor(e_next, device=self.device, dtype=torch.float32)\n",
    "        head_emb = head_emb.to(self.device)\n",
    "        b_emb = b_emb.to(self.device)\n",
    "\n",
    "        # inverse decoder -> s_hat (normalized)\n",
    "        _, s_hat = self.inverse_decoder(e_next, b_emb, head_emb)\n",
    "\n",
    "        return e_last, s_hat, enc_loss, logits_pos, per_step_logits\n",
    "\n",
    "    def _encode_query(self, Bpos, lookup_df, nid2body, tokenizer, max_len):\n",
    "        \"\"\"Encodes the query/doc_text for Bpos on the T5 encoder using model device.\"\"\"\n",
    "        row = lookup_df.loc[Bpos]\n",
    "        head_id = row['Head']\n",
    "        tail_id = row['Tail']\n",
    "        doc_text = nid2body.get(head_id, \"\")\n",
    "        query_text = \"Generate headline for this: \" + doc_text\n",
    "\n",
    "        # Tokenize and run encoder on the same device as the T5 model\n",
    "        inputs = tokenizer(query_text, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=\"max_length\")\n",
    "        model_device = next(self.t5.parameters()).device\n",
    "        input_ids = inputs.input_ids.to(model_device)\n",
    "        attention_mask = inputs.attention_mask.to(model_device)\n",
    "        enc_outs = self.t5.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        base_enc = enc_outs.last_hidden_state  # [1, seq_len, d_model]\n",
    "        return base_enc, doc_text, tail_id, attention_mask\n",
    "\n",
    "    def _inject_context(self, base_enc, e_last, s_hat):\n",
    "        \"\"\"Injects personalization context into base_enc conservatively and safely.\"\"\"\n",
    "        # e_last -> batch\n",
    "        device = base_enc.device\n",
    "        if e_last is None:\n",
    "            e_last_batch = torch.zeros((base_enc.size(0), self.hidden_dim), device=device)\n",
    "        elif isinstance(e_last, torch.Tensor) and e_last.dim() == 1:\n",
    "            e_last_batch = e_last.unsqueeze(0).to(device)\n",
    "        elif isinstance(e_last, torch.Tensor):\n",
    "            e_last_batch = e_last.to(device)\n",
    "        else:\n",
    "            # fallback to zeros if e_last is unexpected type\n",
    "            e_last_batch = torch.zeros((base_enc.size(0), self.hidden_dim), device=device)\n",
    "\n",
    "        gate = torch.sigmoid(self.e_proj(e_last_batch) / 10.0)\n",
    "        gate = gate.unsqueeze(1).expand(-1, base_enc.size(1), -1)\n",
    "        gated_enc = base_enc * gate\n",
    "\n",
    "        if s_hat is None:\n",
    "            s_hat_batch = torch.zeros((base_enc.size(0), self.hidden_dim), device=device)\n",
    "        elif isinstance(s_hat, torch.Tensor) and s_hat.dim() == 1:\n",
    "            s_hat_batch = s_hat.unsqueeze(0).to(device)\n",
    "        elif isinstance(s_hat, torch.Tensor):\n",
    "            s_hat_batch = s_hat.to(device)\n",
    "        else:\n",
    "            s_hat_batch = torch.zeros((base_enc.size(0), self.hidden_dim), device=device)\n",
    "\n",
    "        K = self.k_proj(s_hat_batch).unsqueeze(1)  # [B, 1, d_model]\n",
    "        V = self.v_proj(s_hat_batch).unsqueeze(1)  # [B, 1, d_model]\n",
    "        Q = self.q_proj(gated_enc)                 # [B, L, d_model]\n",
    "\n",
    "        attn_scores = torch.matmul(Q / (self.t5_hidden_dim ** 0.5), K.transpose(-1, -2))  # [B, L, 1]\n",
    "        attn_scores = torch.clamp(attn_scores, -10, 10)\n",
    "        attn_wts = F.softmax(attn_scores, dim=1)\n",
    "        ctx = torch.matmul(attn_wts, V)  # [B, L, d_model]\n",
    "\n",
    "        ctx_scale = torch.clamp(F.softplus(self.ctx_scale_raw), max=1.0)  # conservative clamp\n",
    "        personalized_enc = gated_enc + 0.1 * ctx_scale * ctx  # extra conservative multiplier\n",
    "\n",
    "        # safety fallback if non-finite values appear\n",
    "        if not torch.isfinite(personalized_enc).all():\n",
    "            personalized_enc = gated_enc\n",
    "        return personalized_enc\n",
    "\n",
    "    def forward(self, Bhist, Bhist_len, Bpos, lookup_df, embed_tables, sid2sum, nid2body, tokenizer, max_len=50, mode=None, gen_kwargs=None):\n",
    "        \"\"\"\n",
    "        Bhist: list of event ids (history)\n",
    "        Bhist_len: int\n",
    "        Bpos: target event id\n",
    "        Returns dict for train or ar mode.\n",
    "        \"\"\"\n",
    "        # 1) behavior encode -> get s_hat and encoder loss\n",
    "        e_last, s_hat, enc_loss, logits_pos, per_step_logits = self._behavior_encode(Bhist, Bhist_len, Bpos, lookup_df, embed_tables)\n",
    "\n",
    "        # 2) encode document / query\n",
    "        base_enc, doc_text, tail_id, base_attention_mask = self._encode_query(Bpos, lookup_df, nid2body, tokenizer, max_len)\n",
    "\n",
    "        # 3) gold summary retrieval (defensive)\n",
    "        raw_gold = sid2sum.get(tail_id, \"\")\n",
    "        # treat None, NaN, non-string or empty string as missing\n",
    "        if raw_gold is None or pd.isna(raw_gold) or (not isinstance(raw_gold, str)) or (str(raw_gold).strip() == \"\"):\n",
    "            gold_summary_text = \"\"\n",
    "        else:\n",
    "            gold_summary_text = str(raw_gold).strip()\n",
    "\n",
    "        personalized_enc = self._inject_context(base_enc, e_last, s_hat)\n",
    "\n",
    "        if mode is None:\n",
    "            mode = \"train\" if self.training else \"ar\"\n",
    "        mode = mode.lower()\n",
    "\n",
    "        model_device = next(self.t5.parameters()).device\n",
    "\n",
    "        # Ensure enc_loss is a tensor on model_device\n",
    "        if not isinstance(enc_loss, torch.Tensor):\n",
    "            try:\n",
    "                enc_loss = torch.tensor(float(enc_loss), device=model_device)\n",
    "            except Exception:\n",
    "                enc_loss = torch.tensor(0.0, device=model_device)\n",
    "        else:\n",
    "            enc_loss = enc_loss.to(model_device)\n",
    "\n",
    "        # TRAIN / TEACHER FORCING\n",
    "        if mode in (\"train\", \"tf\", \"teacher\", \"teacher_forcing\"):\n",
    "            if gold_summary_text == \"\":\n",
    "                return {\n",
    "                    \"skip\": True,\n",
    "                    \"reason\": \"empty_gold\",\n",
    "                    \"total_loss\": torch.tensor(0., device=model_device),\n",
    "                    \"enc_loss\": enc_loss,\n",
    "                    \"gen_loss\": torch.tensor(0., device=model_device),\n",
    "                    \"lm_logits\": None,\n",
    "                    \"doc_text\": doc_text,\n",
    "                    \"gold_summary\": \"\",\n",
    "                    \"pred_summary_tf\": \"\"\n",
    "                }\n",
    "\n",
    "            # prepare tokens and move to model device\n",
    "            gold_tokens = tokenizer(gold_summary_text, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=\"max_length\")\n",
    "            decoder_input_ids = gold_tokens.input_ids[:, :-1].to(model_device)\n",
    "            labels = gold_tokens.input_ids[:, 1:].to(model_device)\n",
    "            pad_id = tokenizer.pad_token_id\n",
    "\n",
    "            # if labels are all pad -> skip\n",
    "            if (labels != pad_id).sum().item() == 0:\n",
    "                return {\"skip\": True, \"reason\": \"all_pad_labels\", \"total_loss\": torch.tensor(0., device=model_device), \"enc_loss\": enc_loss, \"gen_loss\": torch.tensor(0., device=model_device), \"lm_logits\": None, \"doc_text\": doc_text, \"gold_summary\": gold_summary_text, \"pred_summary_tf\": \"\"}\n",
    "\n",
    "            # forward through T5 using our personalized encoder outputs\n",
    "            personalized_enc = personalized_enc.to(model_device).to(torch.float32)\n",
    "            enc_out = BaseModelOutput(last_hidden_state=personalized_enc)\n",
    "            t5_out = self.t5(encoder_outputs=enc_out, decoder_input_ids=decoder_input_ids, labels=labels, return_dict=True, use_cache=False)\n",
    "            gen_loss = t5_out.loss.to(model_device)\n",
    "            lm_logits = torch.clamp(t5_out.logits, -50, 50)\n",
    "\n",
    "            # mix losses conservatively\n",
    "            total_loss = self.enc_loss_weight * enc_loss + (1.0 - self.enc_loss_weight) * gen_loss\n",
    "\n",
    "            # TF decode: argmax then trim to non-pad label length\n",
    "            pred_ids = torch.argmax(lm_logits, dim=-1)\n",
    "            pred_ids_list = pred_ids.detach().cpu().tolist()\n",
    "            nonpad_mask = (labels != pad_id)\n",
    "            if nonpad_mask.any():\n",
    "                valid_len = int(nonpad_mask.sum().item())\n",
    "                pred_ids_trimmed = pred_ids_list[0][:valid_len]\n",
    "            else:\n",
    "                pred_ids_trimmed = pred_ids_list[0]\n",
    "            pred_summary_tf = tokenizer.decode(pred_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "            return {\n",
    "                \"skip\": False,\n",
    "                \"total_loss\": total_loss,\n",
    "                \"enc_loss\": enc_loss,\n",
    "                \"gen_loss\": gen_loss,\n",
    "                \"lm_logits\": lm_logits,\n",
    "                \"doc_text\": doc_text,\n",
    "                \"gold_summary\": gold_summary_text,\n",
    "                \"pred_summary_tf\": pred_summary_tf,\n",
    "                \"logits_pos\": logits_pos,\n",
    "                \"per_step_logits\": per_step_logits\n",
    "            }\n",
    "\n",
    "        # EVAL / AUTOREGRESSIVE\n",
    "        elif mode in (\"eval\", \"test\", \"ar\", \"greedy\", \"beam\"):\n",
    "            personalized_enc = personalized_enc.to(model_device).to(torch.float32)\n",
    "            batch_size, seq_len, _ = personalized_enc.size()\n",
    "            if base_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones((batch_size, seq_len), dtype=torch.long, device=model_device)\n",
    "            else:\n",
    "                encoder_attention_mask = base_attention_mask.to(model_device)\n",
    "\n",
    "            enc_out = BaseModelOutput(last_hidden_state=personalized_enc)\n",
    "\n",
    "            # fallback decoder start id\n",
    "            start_id = getattr(self.t5.config, \"decoder_start_token_id\", None)\n",
    "            if start_id is None:\n",
    "                start_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "            gen_args = dict(\n",
    "                encoder_outputs=enc_out,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                max_length=max_len,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                decoder_start_token_id=start_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            if gen_kwargs:\n",
    "                gen_args.update(gen_kwargs)\n",
    "\n",
    "            try:\n",
    "                generated_ids = self.t5.generate(**gen_args)\n",
    "                if generated_ids is None or generated_ids.size(0) == 0:\n",
    "                    pred_summary_ar = \"\"\n",
    "                else:\n",
    "                    pred_summary_ar = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            except Exception:\n",
    "                # fallback greedy autoregressive loop using decoder + lm_head\n",
    "                pred_summary_ar = \"\"\n",
    "                start_local = start_id if start_id is not None else tokenizer.eos_token_id\n",
    "                decoder_input = torch.tensor([[start_local]], device=model_device)\n",
    "                eos_id = tokenizer.eos_token_id\n",
    "                for _ in range(max_len - 1):\n",
    "                    outputs = self.t5.decoder(input_ids=decoder_input, encoder_hidden_states=personalized_enc, encoder_attention_mask=encoder_attention_mask, return_dict=True)\n",
    "                    lm_logits = self.t5.lm_head(outputs.last_hidden_state)\n",
    "                    next_id = torch.argmax(lm_logits[:, -1, :], dim=-1, keepdim=True)\n",
    "                    decoder_input = torch.cat([decoder_input, next_id], dim=1)\n",
    "                    if eos_id is not None and next_id.item() == eos_id:\n",
    "                        break\n",
    "                pred_summary_ar = tokenizer.decode(decoder_input[0, 1:].detach().cpu().tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "            return {\n",
    "                \"pred_summary_ar\": pred_summary_ar,\n",
    "                \"enc_loss\": enc_loss,\n",
    "                \"doc_text\": doc_text,\n",
    "                \"gold_summary\": gold_summary_text,\n",
    "                \"logits_pos\": logits_pos,\n",
    "                \"per_step_logits\": per_step_logits\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode '{mode}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d7740d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UserID</th>\n",
       "      <th>EHist</th>\n",
       "      <th>EPos</th>\n",
       "      <th>Bhist_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NT1_1</td>\n",
       "      <td>[E124, E125, E126, E127, E128, E129, E130, E13...</td>\n",
       "      <td>E274</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NT1_2</td>\n",
       "      <td>[E126, E127, E128, E129, E130, E131, E132, E13...</td>\n",
       "      <td>E276</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NT1_3</td>\n",
       "      <td>[E128, E129, E130, E131, E132, E133, E134, E13...</td>\n",
       "      <td>E278</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NT1_4</td>\n",
       "      <td>[E130, E131, E132, E133, E134, E135, E136, E13...</td>\n",
       "      <td>E280</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NT1_5</td>\n",
       "      <td>[E132, E133, E134, E135, E136, E137, E138, E13...</td>\n",
       "      <td>E282</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20598</th>\n",
       "      <td>20598</td>\n",
       "      <td>NT99_196</td>\n",
       "      <td>[E70418, E70419, E70420, E70421, E70422, E7042...</td>\n",
       "      <td>E70568</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20599</th>\n",
       "      <td>20599</td>\n",
       "      <td>NT99_197</td>\n",
       "      <td>[E70420, E70421, E70422, E70423, E70424, E7042...</td>\n",
       "      <td>E70570</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20600</th>\n",
       "      <td>20600</td>\n",
       "      <td>NT99_198</td>\n",
       "      <td>[E70422, E70423, E70424, E70425, E70426, E7042...</td>\n",
       "      <td>E70572</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20601</th>\n",
       "      <td>20601</td>\n",
       "      <td>NT99_199</td>\n",
       "      <td>[E70424, E70425, E70426, E70427, E70428, E7042...</td>\n",
       "      <td>E70574</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20602</th>\n",
       "      <td>20602</td>\n",
       "      <td>NT99_200</td>\n",
       "      <td>[E70426, E70427, E70428, E70429, E70430, E7043...</td>\n",
       "      <td>E70576</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20603 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    UserID  \\\n",
       "0               0     NT1_1   \n",
       "1               1     NT1_2   \n",
       "2               2     NT1_3   \n",
       "3               3     NT1_4   \n",
       "4               4     NT1_5   \n",
       "...           ...       ...   \n",
       "20598       20598  NT99_196   \n",
       "20599       20599  NT99_197   \n",
       "20600       20600  NT99_198   \n",
       "20601       20601  NT99_199   \n",
       "20602       20602  NT99_200   \n",
       "\n",
       "                                                   EHist    EPos  Bhist_len  \n",
       "0      [E124, E125, E126, E127, E128, E129, E130, E13...    E274        150  \n",
       "1      [E126, E127, E128, E129, E130, E131, E132, E13...    E276        150  \n",
       "2      [E128, E129, E130, E131, E132, E133, E134, E13...    E278        150  \n",
       "3      [E130, E131, E132, E133, E134, E135, E136, E13...    E280        150  \n",
       "4      [E132, E133, E134, E135, E136, E137, E138, E13...    E282        150  \n",
       "...                                                  ...     ...        ...  \n",
       "20598  [E70418, E70419, E70420, E70421, E70422, E7042...  E70568        150  \n",
       "20599  [E70420, E70421, E70422, E70423, E70424, E7042...  E70570        150  \n",
       "20600  [E70422, E70423, E70424, E70425, E70426, E7042...  E70572        150  \n",
       "20601  [E70424, E70425, E70426, E70427, E70428, E7042...  E70574        150  \n",
       "20602  [E70426, E70427, E70428, E70429, E70430, E7043...  E70576        150  \n",
       "\n",
       "[20603 rows x 5 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "82d68cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Row 0 ===\n",
      "EPos (target id): E274; Bhist_len (true timestep): 150\n",
      "\n",
      "[BehaviorEncoder]\n",
      "  enc_loss (norm if implemented): 744.849182\n",
      "  encoder_total_loss (returned): 4.890811\n",
      "  true timestep (target_pos): 150\n",
      "  predicted timestep (argmax logits): 54\n",
      "  top predictions (pos_idx : score): [(54, 0.5012978911399841), (133, 0.4483029246330261), (49, 0.3906280994415283), (76, 0.3643602132797241), (130, 0.34711331129074097)]\n",
      "\n",
      "[PersonalizedT5Summarizer - TEACHER FORCING]\n",
      "  enc_loss: 744.8491821289062\n",
      "  gen_loss: 5.545691\n",
      "  total_loss: 79.476036\n",
      "  gold_summary (first 200 chars): Legal battle looms over Trump EPA's rule change of Obama's Clean Power Plan rule\n",
      "  pred_summary_tf (teacher-forced): Li-    t    s    (\n",
      "\n",
      "[PersonalizedT5Summarizer - AUTOREGRESSIVE]\n",
      "  pred_summary_ar (generated): .. Generate headline for this story:. Generate headline for this story:.............. The\n",
      "  gold_summary (first 200 chars): Legal battle looms over Trump EPA's rule change of Obama's Clean Power Plan rule\n",
      "  enc_loss (from encoder during AR): tensor(744.8492, device='cuda:0')\n",
      "\n",
      "=== Row 1 ===\n",
      "EPos (target id): E276; Bhist_len (true timestep): 150\n",
      "\n",
      "[BehaviorEncoder]\n",
      "  enc_loss (norm if implemented): 742.214294\n",
      "  encoder_total_loss (returned): 4.907720\n",
      "  true timestep (target_pos): 150\n",
      "  predicted timestep (argmax logits): 137\n",
      "  top predictions (pos_idx : score): [(137, 0.37058308720588684), (26, 0.3247690200805664), (96, 0.30679452419281006), (76, 0.29928797483444214), (75, 0.2599131762981415)]\n",
      "\n",
      "[PersonalizedT5Summarizer - TEACHER FORCING]\n",
      "  enc_loss: 742.2142944335938\n",
      "  gen_loss: 5.430875\n",
      "  total_loss: 79.109215\n",
      "  gold_summary (first 200 chars): Wise choices for stylish updating of old homes\n",
      "  pred_summary_tf (teacher-forced): Wi  Wi for   .\n",
      "\n",
      "[PersonalizedT5Summarizer - AUTOREGRESSIVE]\n",
      "  pred_summary_ar (generated): Generate headline for this article. Generate headline for this article. Generate headline for this article. Generate headline for this article. Generate headline for this article. Generate headline for this article. Generate headline for this article\n",
      "  gold_summary (first 200 chars): Wise choices for stylish updating of old homes\n",
      "  enc_loss (from encoder during AR): tensor(742.2143, device='cuda:0')\n",
      "\n",
      "=== Row 2 ===\n",
      "EPos (target id): E278; Bhist_len (true timestep): 150\n",
      "\n",
      "[BehaviorEncoder]\n",
      "  enc_loss (norm if implemented): 738.202209\n",
      "  encoder_total_loss (returned): 4.913998\n",
      "  true timestep (target_pos): 150\n",
      "  predicted timestep (argmax logits): 137\n",
      "  top predictions (pos_idx : score): [(137, 0.5185238718986511), (99, 0.4127585291862488), (47, 0.39185667037963867), (142, 0.38035932183265686), (12, 0.3768373429775238)]\n",
      "\n",
      "[PersonalizedT5Summarizer - TEACHER FORCING]\n",
      "  enc_loss: 738.2022094726562\n",
      "  gen_loss: 4.014088\n",
      "  total_loss: 77.432899\n",
      "  gold_summary (first 200 chars): Verlander may be reconsidering his stance on MLBs juicing balls\n",
      "  pred_summary_tf (teacher-forced): ..  '. \",, \" .\n",
      "\n",
      "[PersonalizedT5Summarizer - AUTOREGRESSIVE]\n",
      "  pred_summary_ar (generated): \n",
      "  gold_summary (first 200 chars): Verlander may be reconsidering his stance on MLBs juicing balls\n",
      "  enc_loss (from encoder during AR): tensor(738.2022, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# ---- Updated forward pass runner that uses EPos and Bhist_len as target timestep ----\n",
    "\n",
    "max_timelen = int(max_len_plus_one)  # from your earlier cell\n",
    "beh_enc = BehaviorEncoder(hidden_dim=hidden_dim, device=device, max_len=max_timelen).to(device)\n",
    "inv_dec = BehaviorInverseDecoderPredict(hidden_dim=hidden_dim, device=device).to(device)\n",
    "\n",
    "personalized_model = PersonalizedT5Summarizer(\n",
    "    hidden_dim=hidden_dim,\n",
    "    t5_model=summarizer_model,\n",
    "    behavior_encoder=beh_enc,\n",
    "    inverse_decoder=inv_dec,\n",
    "    device=device,\n",
    "    learnable_ctx=True\n",
    ").to(device)\n",
    "\n",
    "# ----------------------------\n",
    "# Forward-pass runner for one (or many) rows\n",
    "# ----------------------------\n",
    "def run_forward_pass_on_row(personalized_model, beh_enc, inv_dec, row_index, train_df, lookup_df, embed_tables, sid2sum, nid2body, tokenizer, max_query_len=50, print_all=True):\n",
    "    # resolve row\n",
    "    row = train_df.iloc[row_index]\n",
    "\n",
    "    # EHist must be list\n",
    "    Bhist = row[\"EHist\"] if \"EHist\" in train_df.columns else None\n",
    "    if Bhist is None or not isinstance(Bhist, (list, tuple)):\n",
    "        raise ValueError(\"EHist not found or not a list for this row.\")\n",
    "\n",
    "    Bhist_len = int(row.get(\"Bhist_len\", len(Bhist)))\n",
    "    Bpos = row[\"EPos\"] if \"EPos\" in train_df.columns and pd.notna(row[\"EPos\"]) else Bhist[-1]\n",
    "\n",
    "    if print_all:\n",
    "        print(f\"\\n=== Row {row_index} ===\")\n",
    "        print(f\"EPos (target id): {Bpos}; Bhist_len (true timestep): {Bhist_len}\")\n",
    "\n",
    "    # 1) run behavior encoder standalone for diagnostics (in eval mode)\n",
    "    beh_enc.eval()\n",
    "    with torch.no_grad():\n",
    "        enc_ret = beh_enc(Bhist, Bhist_len, Bpos, lookup_df, embed_tables)\n",
    "        # unpack flexible return length\n",
    "        if len(enc_ret) == 5:\n",
    "            e_last, e_next, logits_pos, enc_loss, total_loss_enc = enc_ret\n",
    "            per_step_logits = None\n",
    "        else:\n",
    "            e_last, e_next, logits_pos, enc_loss, total_loss_enc, per_step_logits = enc_ret\n",
    "\n",
    "        pred_pos = int(torch.argmax(logits_pos, dim=-1).item())\n",
    "        true_pos = int(Bhist_len)\n",
    "\n",
    "        if print_all:\n",
    "            print(\"\\n[BehaviorEncoder]\")\n",
    "            print(f\"  enc_loss (norm if implemented): {float(enc_loss.item() if isinstance(enc_loss, torch.Tensor) else enc_loss):.6f}\")\n",
    "            print(f\"  encoder_total_loss (returned): {float(total_loss_enc.item() if isinstance(total_loss_enc, torch.Tensor) else total_loss_enc):.6f}\")\n",
    "            print(f\"  true timestep (target_pos): {true_pos}\")\n",
    "            print(f\"  predicted timestep (argmax logits): {pred_pos}\")\n",
    "            try:\n",
    "                topk = torch.topk(logits_pos.squeeze(0), k=min(5, logits_pos.size(-1)))\n",
    "                print(\"  top predictions (pos_idx : score):\", list(zip(topk.indices.tolist(), [float(x) for x in topk.values.tolist()])))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 2) run personalized model in TF (train) mode\n",
    "    personalized_model.train()\n",
    "    out_tf = personalized_model(\n",
    "        Bhist=Bhist,\n",
    "        Bhist_len=Bhist_len,\n",
    "        Bpos=Bpos,\n",
    "        lookup_df=lookup_df,\n",
    "        embed_tables=embed_tables,\n",
    "        sid2sum=sid2sum,\n",
    "        nid2body=nid2body,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_query_len,\n",
    "        mode=\"train\",\n",
    "        gen_kwargs=None\n",
    "    )\n",
    "\n",
    "    if print_all:\n",
    "        print(\"\\n[PersonalizedT5Summarizer - TEACHER FORCING]\")\n",
    "        if out_tf.get(\"skip\", False):\n",
    "            print(f\"  Skipped: {out_tf.get('reason', '<no reason>')}\")\n",
    "        else:\n",
    "            print(f\"  enc_loss: {out_tf['enc_loss']}\")\n",
    "            print(f\"  gen_loss: {out_tf['gen_loss'].item():.6f}\")\n",
    "            print(f\"  total_loss: {out_tf['total_loss'].item():.6f}\")\n",
    "            print(\"  gold_summary (first 200 chars):\", out_tf['gold_summary'][:200])\n",
    "            print(\"  pred_summary_tf (teacher-forced):\", out_tf['pred_summary_tf'][:400])\n",
    "\n",
    "    # 3) run AR generation (eval)\n",
    "    personalized_model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_ar = personalized_model(\n",
    "            Bhist=Bhist,\n",
    "            Bhist_len=Bhist_len,\n",
    "            Bpos=Bpos,\n",
    "            lookup_df=lookup_df,\n",
    "            embed_tables=embed_tables,\n",
    "            sid2sum=sid2sum,\n",
    "            nid2body=nid2body,\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=max_query_len,\n",
    "            mode=\"ar\",\n",
    "            gen_kwargs={\"max_length\": max_query_len, \"num_beams\": 1}\n",
    "        )\n",
    "\n",
    "    if print_all:\n",
    "        print(\"\\n[PersonalizedT5Summarizer - AUTOREGRESSIVE]\")\n",
    "        print(\"  pred_summary_ar (generated):\", out_ar.get(\"pred_summary_ar\", \"\"))\n",
    "        print(\"  gold_summary (first 200 chars):\", out_ar.get(\"gold_summary\", \"\")[:200])\n",
    "        print(\"  enc_loss (from encoder during AR):\", out_ar.get(\"enc_loss\"))\n",
    "\n",
    "    return {\"row\": row_index, \"beh_enc\": {\"pred_pos\": pred_pos, \"true_pos\": true_pos, \"enc_loss\": enc_loss}, \"tf\": out_tf, \"ar\": out_ar}\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage: run for first 3 rows\n",
    "# ----------------------------\n",
    "# instantiate the wrapped model (if not done already)\n",
    "beh_enc = behavior_encoder  # your existing instance\n",
    "inv_dec = inverse_decoder   # your existing instance\n",
    "personalized_model = PersonalizedT5Summarizer(hidden_dim=hidden_dim, t5_model=summarizer_model, behavior_encoder=beh_enc, inverse_decoder=inv_dec, device=device, learnable_ctx=True).to(next(summarizer_model.parameters()).device)\n",
    "\n",
    "results = []\n",
    "for i in range(min(3, len(train_df))):\n",
    "    try:\n",
    "        res = run_forward_pass_on_row(personalized_model, beh_enc, inv_dec, i, train_df, lookup_df, embed_tables, sid2sum, nid2body, tokenizer, max_query_len=50, print_all=True)\n",
    "        results.append(res)\n",
    "    except Exception as e:\n",
    "        print(f\"Row {i} failed with error: {repr(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2ac42fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda; total trainable params: 61,971,630\n",
      "\n",
      "Trainable parameter breakdown (name : count):\n",
      "  ctx_scale_raw : 1\n",
      "  t5.shared.weight : 24,674,304\n",
      "  t5.decoder.block.0.layer.1.EncDecAttention.q.weight : 589,824\n",
      "  t5.decoder.block.0.layer.1.EncDecAttention.k.weight : 589,824\n",
      "  t5.decoder.block.0.layer.1.EncDecAttention.v.weight : 589,824\n",
      "  t5.decoder.block.0.layer.1.EncDecAttention.o.weight : 589,824\n",
      "  t5.decoder.block.1.layer.1.EncDecAttention.q.weight : 589,824\n",
      "  t5.decoder.block.1.layer.1.EncDecAttention.k.weight : 589,824\n",
      "  t5.decoder.block.1.layer.1.EncDecAttention.v.weight : 589,824\n",
      "  t5.decoder.block.1.layer.1.EncDecAttention.o.weight : 589,824\n",
      "  t5.decoder.block.2.layer.1.EncDecAttention.q.weight : 589,824\n",
      "  t5.decoder.block.2.layer.1.EncDecAttention.k.weight : 589,824\n",
      "  t5.decoder.block.2.layer.1.EncDecAttention.v.weight : 589,824\n",
      "  t5.decoder.block.2.layer.1.EncDecAttention.o.weight : 589,824\n",
      "  t5.decoder.block.3.layer.1.EncDecAttention.q.weight : 589,824\n",
      "  t5.decoder.block.3.layer.1.EncDecAttention.k.weight : 589,824\n",
      "  t5.decoder.block.3.layer.1.EncDecAttention.v.weight : 589,824\n",
      "  t5.decoder.block.3.layer.1.EncDecAttention.o.weight : 589,824\n",
      "  t5.decoder.block.4.layer.1.EncDecAttention.q.weight : 589,824\n",
      "  t5.decoder.block.4.layer.1.EncDecAttention.k.weight : 589,824\n",
      "  t5.decoder.block.4.layer.1.EncDecAttention.v.weight : 589,824\n",
      "  t5.decoder.block.4.layer.1.EncDecAttention.o.weight : 589,824\n",
      "  t5.decoder.block.5.layer.1.EncDecAttention.q.weight : 589,824\n",
      "  t5.decoder.block.5.layer.1.EncDecAttention.k.weight : 589,824\n",
      "  t5.decoder.block.5.layer.1.EncDecAttention.v.weight : 589,824\n",
      "  t5.decoder.block.5.layer.1.EncDecAttention.o.weight : 589,824\n",
      "  t5.decoder.block.6.layer.1.EncDecAttention.q.weight : 589,824\n",
      "  t5.decoder.block.6.layer.1.EncDecAttention.k.weight : 589,824\n",
      "  t5.decoder.block.6.layer.1.EncDecAttention.v.weight : 589,824\n",
      "  t5.decoder.block.6.layer.1.EncDecAttention.o.weight : 589,824\n",
      "  t5.decoder.block.7.layer.1.EncDecAttention.q.weight : 589,824\n",
      "  t5.decoder.block.7.layer.1.EncDecAttention.k.weight : 589,824\n",
      "  t5.decoder.block.7.layer.1.EncDecAttention.v.weight : 589,824\n",
      "  t5.decoder.block.7.layer.1.EncDecAttention.o.weight : 589,824\n",
      "  t5.decoder.block.8.layer.1.EncDecAttention.q.weight : 589,824\n",
      "  t5.decoder.block.8.layer.1.EncDecAttention.k.weight : 589,824\n",
      "  t5.decoder.block.8.layer.1.EncDecAttention.v.weight : 589,824\n",
      "  t5.decoder.block.8.layer.1.EncDecAttention.o.weight : 589,824\n",
      "  t5.decoder.block.9.layer.1.EncDecAttention.q.weight : 589,824\n",
      "  t5.decoder.block.9.layer.1.EncDecAttention.k.weight : 589,824\n",
      "  t5.decoder.block.9.layer.1.EncDecAttention.v.weight : 589,824\n",
      "  t5.decoder.block.9.layer.1.EncDecAttention.o.weight : 589,824\n",
      "  t5.decoder.block.10.layer.1.EncDecAttention.q.weight : 589,824\n",
      "  t5.decoder.block.10.layer.1.EncDecAttention.k.weight : 589,824\n",
      "  t5.decoder.block.10.layer.1.EncDecAttention.v.weight : 589,824\n",
      "  t5.decoder.block.10.layer.1.EncDecAttention.o.weight : 589,824\n",
      "  t5.decoder.block.11.layer.1.EncDecAttention.q.weight : 589,824\n",
      "  t5.decoder.block.11.layer.1.EncDecAttention.k.weight : 589,824\n",
      "  t5.decoder.block.11.layer.1.EncDecAttention.v.weight : 589,824\n",
      "  t5.decoder.block.11.layer.1.EncDecAttention.o.weight : 589,824\n",
      "  behavior_encoder.e_clk : 4\n",
      "  behavior_encoder.e_skp : 4\n",
      "  behavior_encoder.e_gensumm : 4\n",
      "  behavior_encoder.e_sumgen : 4\n",
      "  behavior_encoder.b_z : 3\n",
      "  behavior_encoder.b_emb : 768\n",
      "  behavior_encoder.alpha : 1\n",
      "  behavior_encoder.beta : 1\n",
      "  behavior_encoder.W_clk.weight : 3,072\n",
      "  behavior_encoder.W_skp.weight : 3,072\n",
      "  behavior_encoder.W_gensumm.weight : 3,072\n",
      "  behavior_encoder.W_sumgen.weight : 3,072\n",
      "  behavior_encoder.W_pull.weight : 768\n",
      "  behavior_encoder.W_s.weight : 589,824\n",
      "  behavior_encoder.W_d.weight : 589,824\n",
      "  behavior_encoder.Wh.weight : 589,824\n",
      "  behavior_encoder.Wc.weight : 589,824\n",
      "  behavior_encoder.Wz.weight : 2,304\n",
      "  behavior_encoder.W_emb.weight : 589,824\n",
      "  behavior_encoder.W_emb.bias : 768\n",
      "  behavior_encoder.W_angle.weight : 589,824\n",
      "  behavior_encoder.W_theta.weight : 768\n",
      "  behavior_encoder.W_h.weight : 589,824\n",
      "  behavior_encoder.W_m.weight : 768\n",
      "  behavior_encoder.classifier.weight : 116,736\n",
      "  behavior_encoder.classifier.bias : 152\n",
      "  behavior_encoder.W_next.weight : 589,824\n",
      "  inverse_decoder.W_emb_pinv.weight : 589,824\n",
      "  inverse_decoder.W1_pinv.weight : 589,824\n",
      "  inverse_decoder.W2_pinv.weight : 589,824\n",
      "  e_proj.weight : 589,824\n",
      "  e_proj.bias : 768\n",
      "  q_proj.weight : 589,824\n",
      "  q_proj.bias : 768\n",
      "  k_proj.weight : 589,824\n",
      "  k_proj.bias : 768\n",
      "  v_proj.weight : 589,824\n",
      "  v_proj.bias : 768\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - dataset, dataloader, model instantiation, trainable param count\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Dataset + collate_fn (unchanged; keeps Bhist as list per sample)\n",
    "class BehaviorDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        Bhist = row[\"EHist\"]\n",
    "        Bhist_len = int(row.get(\"Bhist_len\", len(Bhist)))\n",
    "        Bpos = row[\"EPos\"] if \"EPos\" in row.index and pd.notna(row[\"EPos\"]) else Bhist[-1]\n",
    "        return {\"Bhist\": Bhist, \"Bhist_len\": Bhist_len, \"Bpos\": Bpos, \"idx\": idx}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    Bhist_list = [item[\"Bhist\"] for item in batch]\n",
    "    Bhist_len_list = [int(item[\"Bhist_len\"]) for item in batch]\n",
    "    Bpos_list = [item[\"Bpos\"] for item in batch]\n",
    "    idxs = [item[\"idx\"] for item in batch]\n",
    "    return {\"Bhist\": Bhist_list, \"Bhist_len\": Bhist_len_list, \"Bpos\": Bpos_list, \"idx\": idxs}\n",
    "\n",
    "# instantiate dataset and loader\n",
    "dataset = BehaviorDataset(train_df)  # uses train_df from your notebook\n",
    "batch_size = 8\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "# instantiate / reuse model objects (uses your existing instances)\n",
    "beh_enc = behavior_encoder   # should exist in your session\n",
    "inv_dec = inverse_decoder    # should exist in your session\n",
    "\n",
    "device_model = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "personalized_model = PersonalizedT5Summarizer(\n",
    "    hidden_dim=hidden_dim,\n",
    "    t5_model=summarizer_model,\n",
    "    behavior_encoder=beh_enc,\n",
    "    inverse_decoder=inv_dec,\n",
    "    device=device_model,\n",
    "    learnable_ctx=True\n",
    ")\n",
    "personalized_model.to(device_model)\n",
    "\n",
    "# compute trainable params by group (per-parameter)\n",
    "trainable = [p for p in personalized_model.parameters() if p.requires_grad]\n",
    "total_trainable = sum(p.numel() for p in trainable)\n",
    "print(f\"Model device: {device_model}; total trainable params: {total_trainable:,}\")\n",
    "\n",
    "# Also print a per-module breakdown by name (helpful)\n",
    "print(\"\\nTrainable parameter breakdown (name : count):\")\n",
    "for name, p in personalized_model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(f\"  {name} : {p.numel():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8f4bfd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import time\n",
    "\n",
    "def train_epochs(model, loader, optimizer, scheduler, epochs=3, save_dir=\"checkpoints\", clip_norm=1.0, log_every=50):\n",
    "    \"\"\"\n",
    "    Robust training loop: safe per-sample try/except, skip bad samples,\n",
    "    skip optimizer.step if no valid items in batch, check finite losses.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    global_step = 0\n",
    "    model.to(device_model)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, epochs + 4):\n",
    "        # print trainable param count and per-epoch snapshot\n",
    "        trainable = [p for p in model.parameters() if p.requires_grad]\n",
    "        total_trainable = sum(p.numel() for p in trainable)\n",
    "        print(f\"\\n=== Epoch {epoch}/{epochs} ‚Äî trainable params: {total_trainable:,} ===\")\n",
    "\n",
    "        # running stats for postfix\n",
    "        running_enc_loss = 0.0\n",
    "        running_gen_loss = 0.0\n",
    "        running_total_loss = 0.0\n",
    "        running_items = 0\n",
    "\n",
    "        pbar = tqdm(enumerate(loader), total=len(loader), desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
    "        for step, batch in pbar:\n",
    "            Bhist_batch = batch[\"Bhist\"]\n",
    "            Bhist_len_batch = batch[\"Bhist_len\"]\n",
    "            Bpos_batch = batch[\"Bpos\"]\n",
    "            idxs = batch.get(\"idx\", [None] * len(Bhist_batch))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch_valid_items = 0\n",
    "            batch_loss_sum = 0.0\n",
    "\n",
    "            # iterate micro-samples in this batch\n",
    "            for i in range(len(Bhist_batch)):\n",
    "                Bhist = Bhist_batch[i]\n",
    "                Bhist_len = int(Bhist_len_batch[i])\n",
    "                Bpos = Bpos_batch[i]\n",
    "                idx = idxs[i] if i < len(idxs) else None\n",
    "\n",
    "                # Forward wrapped in try/except to prevent single bad sample from stopping training\n",
    "                try:\n",
    "                    out = model(\n",
    "                        Bhist=Bhist,\n",
    "                        Bhist_len=Bhist_len,\n",
    "                        Bpos=Bpos,\n",
    "                        lookup_df=lookup_df,\n",
    "                        embed_tables=embed_tables,\n",
    "                        sid2sum=sid2sum,\n",
    "                        nid2body=nid2body,\n",
    "                        tokenizer=tokenizer,\n",
    "                        max_len=50,\n",
    "                        mode=\"train\",\n",
    "                        gen_kwargs=None\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    # attempt to get tail_id for logging\n",
    "                    try:\n",
    "                        tail = lookup_df.loc[Bpos]['Tail'] if Bpos in lookup_df.index else None\n",
    "                    except Exception:\n",
    "                        tail = None\n",
    "                    print(f\"[WARN] Forward failed for sample idx={idx} Bpos={Bpos} tail={tail} -> {repr(e)}\")\n",
    "                    traceback.print_exc()\n",
    "                    # skip this sample\n",
    "                    continue\n",
    "\n",
    "                # If model requested to skip (missing gold etc), honor it\n",
    "                if out.get(\"skip\", False):\n",
    "                    continue\n",
    "\n",
    "                # obtain loss (prefer total_loss returned by model)\n",
    "                if \"total_loss\" in out and out[\"total_loss\"] is not None:\n",
    "                    loss = out[\"total_loss\"]\n",
    "                else:\n",
    "                    enc_loss = out.get(\"enc_loss\", torch.tensor(0., device=device_model))\n",
    "                    gen_loss = out.get(\"gen_loss\", torch.tensor(0., device=device_model))\n",
    "                    loss = 0.5 * enc_loss.to(device_model) + 0.5 * gen_loss.to(device_model)\n",
    "\n",
    "                # Ensure loss is a tensor on device\n",
    "                if not isinstance(loss, torch.Tensor):\n",
    "                    try:\n",
    "                        loss = torch.tensor(float(loss), device=device_model)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] Could not convert loss to tensor for idx={idx}, Bpos={Bpos}: {repr(e)}\")\n",
    "                        continue\n",
    "                loss = loss.to(device_model)\n",
    "\n",
    "                # Check finite\n",
    "                if not torch.isfinite(loss):\n",
    "                    print(f\"[WARN] Non-finite loss for idx={idx}, Bpos={Bpos}. Skipping sample.\")\n",
    "                    continue\n",
    "\n",
    "                # BACKWARD within try/except\n",
    "                try:\n",
    "                    # normalize by actual number of valid items we intend to accumulate across the batch\n",
    "                    # we don't know final valid count yet; to keep magnitudes stable normalize by batch size as before\n",
    "                    (loss / float(len(Bhist_batch))).backward()\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Backward failed for idx={idx}, Bpos={Bpos}: {repr(e)}\")\n",
    "                    traceback.print_exc()\n",
    "                    # clear grads to avoid corrupt state and skip rest of items in this batch\n",
    "                    optimizer.zero_grad()\n",
    "                    break  # break out of inner loop, proceed to optimizer step only if we have grads\n",
    "                # success for this sample\n",
    "                batch_valid_items += 1\n",
    "                batch_loss_sum += float(loss.item())\n",
    "\n",
    "                # accumulate running metrics if present\n",
    "                enc_loss_val = out.get(\"enc_loss\", None)\n",
    "                gen_loss_val = out.get(\"gen_loss\", None)\n",
    "                total_loss_val = out.get(\"total_loss\", None)\n",
    "\n",
    "                if isinstance(enc_loss_val, torch.Tensor):\n",
    "                    running_enc_loss += float(enc_loss_val.item())\n",
    "                elif enc_loss_val is not None:\n",
    "                    running_enc_loss += float(enc_loss_val)\n",
    "\n",
    "                if isinstance(gen_loss_val, torch.Tensor):\n",
    "                    running_gen_loss += float(gen_loss_val.item())\n",
    "                elif gen_loss_val is not None:\n",
    "                    running_gen_loss += float(gen_loss_val)\n",
    "\n",
    "                if isinstance(total_loss_val, torch.Tensor):\n",
    "                    running_total_loss += float(total_loss_val.item())\n",
    "                elif total_loss_val is not None:\n",
    "                    running_total_loss += float(total_loss_val)\n",
    "\n",
    "                running_items += 1\n",
    "\n",
    "            # After processing micro-items in batch:\n",
    "            if batch_valid_items == 0:\n",
    "                # Nothing to step on; skip optimizer and scheduler step but continue training\n",
    "                pbar.set_postfix({\"avg_enc\": f\"{(running_enc_loss / max(1, running_items)):.4f}\",\n",
    "                                  \"avg_gen\": f\"{(running_gen_loss / max(1, running_items)):.4f}\",\n",
    "                                  \"avg_total\": f\"{(running_total_loss / max(1, running_items)):.4f}\",\n",
    "                                  \"step\": global_step})\n",
    "                continue\n",
    "\n",
    "            # clip + step wrapped in try/except to avoid fatal crash\n",
    "            try:\n",
    "                torch.nn.utils.clip_grad_norm_(trainable_params, clip_norm)\n",
    "                optimizer.step()\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Optimizer step failed at global_step={global_step}: {repr(e)}\")\n",
    "                traceback.print_exc()\n",
    "                # clear grads and continue\n",
    "                optimizer.zero_grad()\n",
    "                try:\n",
    "                    scheduler.step()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                global_step += 1\n",
    "                continue\n",
    "\n",
    "            # safe scheduler step\n",
    "            try:\n",
    "                scheduler.step()\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Scheduler.step() failed at step {global_step}: {repr(e)}\")\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # update tqdm postfix with averages (avoid division by zero)\n",
    "            avg_enc = running_enc_loss / max(1, running_items)\n",
    "            avg_gen = running_gen_loss / max(1, running_items)\n",
    "            avg_tot = running_total_loss / max(1, running_items)\n",
    "            pbar.set_postfix({\"avg_enc\": f\"{avg_enc:.4f}\", \"avg_gen\": f\"{avg_gen:.4f}\", \"avg_total\": f\"{avg_tot:.4f}\", \"step\": global_step})\n",
    "\n",
    "            # periodic print/log\n",
    "            if global_step % log_every == 0:\n",
    "                print(f\"Step {global_step} -> avg_enc: {avg_enc:.4f}, avg_gen: {avg_gen:.4f}, avg_total: {avg_tot:.4f}\")\n",
    "\n",
    "        # epoch summary and checkpoint\n",
    "        avg_epoch_enc = running_enc_loss / max(1, running_items)\n",
    "        avg_epoch_gen = running_gen_loss / max(1, running_items)\n",
    "        avg_epoch_tot = running_total_loss / max(1, running_items)\n",
    "        print(f\"Epoch {epoch} completed. avg_enc_loss={avg_epoch_enc:.6f}, avg_gen_loss={avg_epoch_gen:.6f}, avg_total_loss={avg_epoch_tot:.6f}\")\n",
    "\n",
    "        ckpt_path = os.path.join(save_dir, f\"personalized_model_epoch{epoch}.pt\")\n",
    "        try:\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }, ckpt_path)\n",
    "            print(\"Saved checkpoint to\", ckpt_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to save checkpoint at epoch {epoch}: {repr(e)}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1ae53082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - evaluation function and run training + eval\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "def evaluate(model, dataset, batch_size=8, max_query_len=50, do_ar=True, max_examples=200):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    loader_eval = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    stats = defaultdict(float)\n",
    "    counts = defaultdict(int)\n",
    "    examples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader_eval, desc=\"Evaluating\"):\n",
    "            Bhist_batch = batch[\"Bhist\"]\n",
    "            Bhist_len_batch = batch[\"Bhist_len\"]\n",
    "            Bpos_batch = batch[\"Bpos\"]\n",
    "            idxs = batch[\"idx\"]\n",
    "\n",
    "            for i in range(len(Bhist_batch)):\n",
    "                Bhist = Bhist_batch[i]\n",
    "                Bhist_len = int(Bhist_len_batch[i])\n",
    "                Bpos = Bpos_batch[i]\n",
    "                idx = idxs[i]\n",
    "\n",
    "                out_tf = model(Bhist=Bhist, Bhist_len=Bhist_len, Bpos=Bpos, lookup_df=lookup_df, embed_tables=embed_tables, sid2sum=sid2sum, nid2body=nid2body, tokenizer=tokenizer, max_len=max_query_len, mode=\"train\")\n",
    "                if out_tf.get(\"skip\", False):\n",
    "                    continue\n",
    "\n",
    "                enc_loss = out_tf.get(\"enc_loss\", 0.0)\n",
    "                gen_loss = out_tf.get(\"gen_loss\", 0.0)\n",
    "                total_loss = out_tf.get(\"total_loss\", gen_loss)\n",
    "\n",
    "                stats[\"enc_loss_sum\"] += float(enc_loss.item() if isinstance(enc_loss, torch.Tensor) else enc_loss)\n",
    "                counts[\"enc_count\"] += 1\n",
    "                stats[\"gen_loss_sum\"] += float(gen_loss.item() if isinstance(gen_loss, torch.Tensor) else gen_loss)\n",
    "                counts[\"gen_count\"] += 1\n",
    "\n",
    "                pred_tf = out_tf.get(\"pred_summary_tf\", \"\")\n",
    "                gold = out_tf.get(\"gold_summary\", \"\")\n",
    "\n",
    "                pred_ar = \"\"\n",
    "                if do_ar:\n",
    "                    out_ar = model(Bhist=Bhist, Bhist_len=Bhist_len, Bpos=Bpos, lookup_df=lookup_df, embed_tables=embed_tables, sid2sum=sid2sum, nid2body=nid2body, tokenizer=tokenizer, max_len=max_query_len, mode=\"ar\", gen_kwargs={\"num_beams\":1, \"max_length\":max_query_len})\n",
    "                    pred_ar = out_ar.get(\"pred_summary_ar\", \"\")\n",
    "\n",
    "                if len(examples) < max_examples:\n",
    "                    examples.append({\"idx\": idx, \"gold\": gold, \"pred_tf\": pred_tf, \"pred_ar\": pred_ar})\n",
    "\n",
    "    avg_enc_loss = stats[\"enc_loss_sum\"] / max(1, counts[\"enc_count\"])\n",
    "    avg_gen_loss = stats[\"gen_loss_sum\"] / max(1, counts[\"gen_count\"])\n",
    "\n",
    "    return {\"avg_enc_loss\": avg_enc_loss, \"avg_gen_loss\": avg_gen_loss, \"examples\": examples}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6a2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/4 ‚Äî trainable params: 61,971,630 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218d58c20f65475fba6c60b3c5ebd609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/4:   0%|          | 0/2576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 -> avg_enc: 664.0659, avg_gen: 3.3408, avg_total: 69.4133\n",
      "Step 200 -> avg_enc: 659.3521, avg_gen: 2.9597, avg_total: 68.5989\n",
      "Step 300 -> avg_enc: 654.8724, avg_gen: 2.7679, avg_total: 67.9783\n",
      "Step 400 -> avg_enc: 650.6414, avg_gen: 2.6555, avg_total: 67.4541\n",
      "Step 500 -> avg_enc: 646.8567, avg_gen: 2.5590, avg_total: 66.9887\n",
      "Step 600 -> avg_enc: 642.4596, avg_gen: 2.4729, avg_total: 66.4716\n",
      "Step 700 -> avg_enc: 638.1135, avg_gen: 2.4064, avg_total: 65.9771\n",
      "Step 800 -> avg_enc: 633.6829, avg_gen: 2.3521, avg_total: 65.4852\n",
      "Step 900 -> avg_enc: 629.9839, avg_gen: 2.3044, avg_total: 65.0724\n",
      "Step 1000 -> avg_enc: 626.0656, avg_gen: 2.2610, avg_total: 64.6414\n",
      "Step 1100 -> avg_enc: 622.0072, avg_gen: 2.2251, avg_total: 64.2033\n",
      "Step 1200 -> avg_enc: 618.9665, avg_gen: 2.1886, avg_total: 63.8663\n",
      "Step 1300 -> avg_enc: 615.2489, avg_gen: 2.1540, avg_total: 63.4635\n",
      "Step 1400 -> avg_enc: 611.9683, avg_gen: 2.1261, avg_total: 63.1103\n",
      "Step 1500 -> avg_enc: 608.6586, avg_gen: 2.0999, avg_total: 62.7558\n",
      "Step 1600 -> avg_enc: 605.9355, avg_gen: 2.0740, avg_total: 62.4602\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Run training for a few epochs then evaluate\n",
    "# -----------------------------\n",
    "# seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# run training (the train_epochs function prints params per-epoch and shows postfix)\n",
    "start_time = time.time()\n",
    "personalized_model = train_epochs(personalized_model, loader, optimizer, scheduler, epochs=4, save_dir=\"checkpoints_small\", clip_norm=1.0, log_every=100)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTraining finished in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "# evaluate (model.eval inside)\n",
    "eval_result = evaluate(personalized_model, dataset, batch_size=30, max_query_len=50, do_ar=True, max_examples=50)\n",
    "print(\"\\nEVAL avg_enc_loss:\", eval_result[\"avg_enc_loss\"], \"avg_gen_loss:\", eval_result[\"avg_gen_loss\"])\n",
    "print(\"Sample predictions:\")\n",
    "for ex in eval_result[\"examples\"][:10]:\n",
    "    print(\"idx\", ex[\"idx\"])\n",
    "    print(\" gold:\", ex[\"gold\"][:150])\n",
    "    print(\" pred_tf:\", ex[\"pred_tf\"][:150])\n",
    "    print(\" pred_ar:\", ex[\"pred_ar\"][:150])\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
