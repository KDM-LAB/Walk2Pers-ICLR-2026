{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d29e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "import pickle\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4bcc80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Load T5-Base Model\n",
    "# ---------------------\n",
    "summarizer_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "summarizer_model.eval()\n",
    "for param in summarizer_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9be291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded nid2body with 113762 items\n",
      "üßæ Sample NID: N10000\n",
      "üìù Headline: Predicting Atlanta United's lineup against Columbus Crew in the U.S. Open Cup\n"
     ]
    }
   ],
   "source": [
    "# Load nid2body from pickle\n",
    "with open(\"nid2body.pkl\", \"rb\") as f:\n",
    "    nid2body = pickle.load(f)\n",
    "\n",
    "# Debug print\n",
    "print(f\"‚úÖ Loaded nid2body with {len(nid2body)} items\")\n",
    "sample_nid = list(nid2body.keys())[0]\n",
    "print(f\"üßæ Sample NID: {sample_nid}\\nüìù Headline: {nid2body[sample_nid][:300]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08fb8c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded sid2sum with 20600 items\n",
      "üßæ Sample SID: SUMM1\n",
      "üìù Summary: Legal battle looms over Trump EPA's rule change of Obama's Clean Power Plan rule\n"
     ]
    }
   ],
   "source": [
    "# Load sid2sum from pickle\n",
    "with open(\"sid2sum_test.pkl\", \"rb\") as f:\n",
    "    sid2sum = pickle.load(f)\n",
    "\n",
    "# Debug print\n",
    "print(f\"‚úÖ Loaded sid2sum with {len(sid2sum)} items\")\n",
    "sample_sid = list(sid2sum.keys())[0]\n",
    "print(f\"üßæ Sample SID: {sample_sid}\\nüìù Summary: {sid2sum[sample_sid][:300]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e60dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Device and Precision Setup ===\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_dim = 768\n",
    "\n",
    "# === Utility Functions ===\n",
    "def get_embedding(key, table, dim):\n",
    "    if key not in table:\n",
    "        table[key] = torch.nn.Parameter(torch.randn(dim, dtype=torch.float32, device=device) * 0.01, requires_grad=True)\n",
    "    return table[key]\n",
    "\n",
    "\n",
    "# === Load Embeddings ===\n",
    "with open(\"testsum_T5.pkl\", \"rb\") as f:\n",
    "    summary_embed = {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in pickle.load(f).items()}\n",
    "with open(\"newsbody_T5.pkl\", \"rb\") as f:\n",
    "    newsbody_embed = {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in pickle.load(f).items()}\n",
    "with open(\"headline_T5.pkl\", \"rb\") as f:\n",
    "    headline_embed = {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in pickle.load(f).items()}\n",
    "\n",
    "embed_tables = {\n",
    "    'summary': summary_embed,\n",
    "    'newsbody': newsbody_embed,\n",
    "    'headline': headline_embed\n",
    "}\n",
    "\n",
    "# === Load Dataset ===\n",
    "lookup_df = pd.read_csv(\"w2p_test_engage_list_updated.csv\").set_index('EdgeID')\n",
    "# train_df = pd.read_csv(\"train_w2p.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083823d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UserID</th>\n",
       "      <th>EHist</th>\n",
       "      <th>EPos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>U10000_1</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>U10000_2</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>U100006_1</td>\n",
       "      <td>['E151']</td>\n",
       "      <td>E152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>U100006_2</td>\n",
       "      <td>['E151', 'E152', 'E153', 'E154', 'E155', 'E156...</td>\n",
       "      <td>E168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>U100006_3</td>\n",
       "      <td>['E151', 'E152', 'E153', 'E154', 'E155', 'E156...</td>\n",
       "      <td>E230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>195583</td>\n",
       "      <td>U233775_3</td>\n",
       "      <td>['E1882808', 'E1882809', 'E1882810', 'E1882811...</td>\n",
       "      <td>E1882859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>195584</td>\n",
       "      <td>U233775_4</td>\n",
       "      <td>['E1882808', 'E1882809', 'E1882810', 'E1882811...</td>\n",
       "      <td>E1882866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>195585</td>\n",
       "      <td>U233775_5</td>\n",
       "      <td>['E1882808', 'E1882809', 'E1882810', 'E1882811...</td>\n",
       "      <td>E1882869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>195586</td>\n",
       "      <td>U233775_6</td>\n",
       "      <td>['E1882808', 'E1882809', 'E1882810', 'E1882811...</td>\n",
       "      <td>E1882920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>195587</td>\n",
       "      <td>U233777_1</td>\n",
       "      <td>['E1882958', 'E1882959', 'E1882960', 'E1882961...</td>\n",
       "      <td>E1883030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     UserID  \\\n",
       "0               2   U10000_1   \n",
       "1               3   U10000_2   \n",
       "2              11  U100006_1   \n",
       "3              12  U100006_2   \n",
       "4              13  U100006_3   \n",
       "...           ...        ...   \n",
       "49995      195583  U233775_3   \n",
       "49996      195584  U233775_4   \n",
       "49997      195585  U233775_5   \n",
       "49998      195586  U233775_6   \n",
       "49999      195587  U233777_1   \n",
       "\n",
       "                                                   EHist      EPos  \n",
       "0      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...       E84  \n",
       "1      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...      E133  \n",
       "2                                               ['E151']      E152  \n",
       "3      ['E151', 'E152', 'E153', 'E154', 'E155', 'E156...      E168  \n",
       "4      ['E151', 'E152', 'E153', 'E154', 'E155', 'E156...      E230  \n",
       "...                                                  ...       ...  \n",
       "49995  ['E1882808', 'E1882809', 'E1882810', 'E1882811...  E1882859  \n",
       "49996  ['E1882808', 'E1882809', 'E1882810', 'E1882811...  E1882866  \n",
       "49997  ['E1882808', 'E1882809', 'E1882810', 'E1882811...  E1882869  \n",
       "49998  ['E1882808', 'E1882809', 'E1882810', 'E1882811...  E1882920  \n",
       "49999  ['E1882958', 'E1882959', 'E1882960', 'E1882961...  E1883030  \n",
       "\n",
       "[50000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df=train_df[:50000]\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe86336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Head</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Tail</th>\n",
       "      <th>User</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EdgeID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NT1</td>\n",
       "      <td>skip</td>\n",
       "      <td>N11866</td>\n",
       "      <td>NT1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>N11866</td>\n",
       "      <td>skip</td>\n",
       "      <td>N81592</td>\n",
       "      <td>NT1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>N81592</td>\n",
       "      <td>click</td>\n",
       "      <td>N52295</td>\n",
       "      <td>NT1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>N52295</td>\n",
       "      <td>click</td>\n",
       "      <td>N100144</td>\n",
       "      <td>NT1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>N100144</td>\n",
       "      <td>skip</td>\n",
       "      <td>N17648</td>\n",
       "      <td>NT1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E70572</th>\n",
       "      <td>NaN</td>\n",
       "      <td>N86284</td>\n",
       "      <td>summ_gen</td>\n",
       "      <td>SUMM19798</td>\n",
       "      <td>NT99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E70573</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SUMM19799</td>\n",
       "      <td>gen_summ</td>\n",
       "      <td>N100284</td>\n",
       "      <td>NT99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E70574</th>\n",
       "      <td>NaN</td>\n",
       "      <td>N100284</td>\n",
       "      <td>summ_gen</td>\n",
       "      <td>SUMM19799</td>\n",
       "      <td>NT99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E70575</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SUMM19800</td>\n",
       "      <td>gen_summ</td>\n",
       "      <td>N57555</td>\n",
       "      <td>NT99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E70576</th>\n",
       "      <td>NaN</td>\n",
       "      <td>N57555</td>\n",
       "      <td>summ_gen</td>\n",
       "      <td>SUMM19800</td>\n",
       "      <td>NT99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70576 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       Head  Relation       Tail  User\n",
       "EdgeID                                                  \n",
       "E1             0.0        NT1      skip     N11866   NT1\n",
       "E2             1.0     N11866      skip     N81592   NT1\n",
       "E3             2.0     N81592     click     N52295   NT1\n",
       "E4             3.0     N52295     click    N100144   NT1\n",
       "E5             4.0    N100144      skip     N17648   NT1\n",
       "...            ...        ...       ...        ...   ...\n",
       "E70572         NaN     N86284  summ_gen  SUMM19798  NT99\n",
       "E70573         NaN  SUMM19799  gen_summ    N100284  NT99\n",
       "E70574         NaN    N100284  summ_gen  SUMM19799  NT99\n",
       "E70575         NaN  SUMM19800  gen_summ     N57555  NT99\n",
       "E70576         NaN     N57555  summ_gen  SUMM19800  NT99\n",
       "\n",
       "[70576 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4651bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UserID</th>\n",
       "      <th>EHist</th>\n",
       "      <th>EPos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NT1_1</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NT1_2</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NT1_3</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NT1_4</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NT1_5</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20598</th>\n",
       "      <td>20598</td>\n",
       "      <td>NT99_196</td>\n",
       "      <td>['E37551', 'E37552', 'E37553', 'E37554', 'E375...</td>\n",
       "      <td>E70568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20599</th>\n",
       "      <td>20599</td>\n",
       "      <td>NT99_197</td>\n",
       "      <td>['E37551', 'E37552', 'E37553', 'E37554', 'E375...</td>\n",
       "      <td>E70570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20600</th>\n",
       "      <td>20600</td>\n",
       "      <td>NT99_198</td>\n",
       "      <td>['E37551', 'E37552', 'E37553', 'E37554', 'E375...</td>\n",
       "      <td>E70572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20601</th>\n",
       "      <td>20601</td>\n",
       "      <td>NT99_199</td>\n",
       "      <td>['E37551', 'E37552', 'E37553', 'E37554', 'E375...</td>\n",
       "      <td>E70574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20602</th>\n",
       "      <td>20602</td>\n",
       "      <td>NT99_200</td>\n",
       "      <td>['E37551', 'E37552', 'E37553', 'E37554', 'E375...</td>\n",
       "      <td>E70576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20603 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    UserID  \\\n",
       "0               0     NT1_1   \n",
       "1               1     NT1_2   \n",
       "2               2     NT1_3   \n",
       "3               3     NT1_4   \n",
       "4               4     NT1_5   \n",
       "...           ...       ...   \n",
       "20598       20598  NT99_196   \n",
       "20599       20599  NT99_197   \n",
       "20600       20600  NT99_198   \n",
       "20601       20601  NT99_199   \n",
       "20602       20602  NT99_200   \n",
       "\n",
       "                                                   EHist    EPos  \n",
       "0      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...    E274  \n",
       "1      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...    E276  \n",
       "2      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...    E278  \n",
       "3      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...    E280  \n",
       "4      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...    E282  \n",
       "...                                                  ...     ...  \n",
       "20598  ['E37551', 'E37552', 'E37553', 'E37554', 'E375...  E70568  \n",
       "20599  ['E37551', 'E37552', 'E37553', 'E37554', 'E375...  E70570  \n",
       "20600  ['E37551', 'E37552', 'E37553', 'E37554', 'E375...  E70572  \n",
       "20601  ['E37551', 'E37552', 'E37553', 'E37554', 'E375...  E70574  \n",
       "20602  ['E37551', 'E37552', 'E37553', 'E37554', 'E375...  E70576  \n",
       "\n",
       "[20603 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df=pd.read_csv(\"w2p_test.csv\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43956d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Tail2Idx from EHist and EPos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting tails: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20603/20603 [01:03<00:00, 322.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tail2Idx (test-only) built with 32501 unique tail IDs.\n",
      "‚ö†Ô∏è Test mapping size = 32501; expected 544137. Merging with train mapping...\n",
      "‚ö†Ô∏è Warning: 26321 test tails not found in train mapping. They will be ignored.\n",
      "‚úÖ Final Tail2Idx size = 544137\n",
      "üíæ Saved tail2idx and idx2tail to tail_mappings_test.pkl\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import pickle\n",
    "# from tqdm import tqdm\n",
    "# from ast import literal_eval\n",
    "\n",
    "# # ---------------------\n",
    "# # Build Tail2Idx from test data\n",
    "# # ---------------------\n",
    "# tail_set = set()\n",
    "\n",
    "# print(\"Building Tail2Idx from EHist and EPos...\")\n",
    "# for row in tqdm(test_df.itertuples(), total=len(test_df), desc=\"Collecting tails\"):\n",
    "#     try:\n",
    "#         bhist = literal_eval(row.EHist)\n",
    "#         bpos = row.EPos\n",
    "\n",
    "#         # Add tails from Bhist\n",
    "#         for b_id in bhist:\n",
    "#             if b_id in lookup_df.index:\n",
    "#                 tail = lookup_df.loc[b_id, 'Tail']\n",
    "#                 tail_set.add(tail)\n",
    "\n",
    "#         # Add tail from Bpos\n",
    "#         if bpos in lookup_df.index:\n",
    "#             tail = lookup_df.loc[bpos, 'Tail']\n",
    "#             tail_set.add(tail)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"[Skip] Error in row {row.Index}: {e}\")\n",
    "\n",
    "# # Build test-only mapping\n",
    "# tail_ids_test = sorted(tail_set)\n",
    "# tail2idx_test = {tid: idx for idx, tid in enumerate(tail_ids_test)}\n",
    "\n",
    "# print(f\"‚úÖ Tail2Idx (test-only) built with {len(tail2idx_test)} unique tail IDs.\")\n",
    "\n",
    "# # ---------------------\n",
    "# # Merge with train mappings if needed\n",
    "# # ---------------------\n",
    "# target_size = 544137  # expected size from training\n",
    "\n",
    "# if len(tail2idx_test) != target_size:\n",
    "#     print(f\"‚ö†Ô∏è Test mapping size = {len(tail2idx_test)}; expected {target_size}. Merging with train mapping...\")\n",
    "\n",
    "#     # Load train mappings\n",
    "#     with open(\"tail_mappings.pkl\", \"rb\") as f:\n",
    "#         train_mappings = pickle.load(f)\n",
    "#     tail2idx_train = train_mappings[\"tail2idx\"]\n",
    "#     idx2tail_train = train_mappings[\"idx2tail\"]\n",
    "\n",
    "#     # Ensure consistent ordering from train\n",
    "#     tail2idx = tail2idx_train.copy()\n",
    "#     idx2tail = idx2tail_train.copy()\n",
    "\n",
    "#     # Just verify all test tails are included\n",
    "#     missing = [tid for tid in tail2idx_test if tid not in tail2idx_train]\n",
    "#     if missing:\n",
    "#         print(f\"‚ö†Ô∏è Warning: {len(missing)} test tails not found in train mapping. They will be ignored.\")\n",
    "\n",
    "# else:\n",
    "#     # Sizes match perfectly\n",
    "#     tail2idx = tail2idx_test\n",
    "#     idx2tail = {idx: tid for tid, idx in tail2idx.items()}\n",
    "\n",
    "# print(f\"‚úÖ Final Tail2Idx size = {len(tail2idx)}\")\n",
    "\n",
    "# # ---------------------\n",
    "# # Save as pickle\n",
    "# # ---------------------\n",
    "# with open(\"tail_mappings_test.pkl\", \"wb\") as f:\n",
    "#     pickle.dump({\"tail2idx\": tail2idx, \"idx2tail\": idx2tail}, f)\n",
    "\n",
    "# print(\"üíæ Saved tail2idx and idx2tail to tail_mappings_test.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca969e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded tail2idx with 544137 entries\n",
      "üßæ Sample tail ‚Üí idx: [('N10000', 0), ('N100001', 1), ('N100003', 2)]\n",
      "üßæ Sample idx ‚Üí tail: [(0, 'N10000'), (1, 'N100001'), (2, 'N100003')]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Load back from pickle\n",
    "# ---------------------\n",
    "with open(\"tail_mappings_test.pkl\", \"rb\") as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "tail2idx = mappings[\"tail2idx\"]\n",
    "idx2tail = mappings[\"idx2tail\"]\n",
    "\n",
    "print(f\"‚úÖ Loaded tail2idx with {len(tail2idx)} entries\")\n",
    "print(f\"üßæ Sample tail ‚Üí idx: {list(tail2idx.items())[:3]}\")\n",
    "print(f\"üßæ Sample idx ‚Üí tail: {list(idx2tail.items())[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5378608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Sequence Engagement/Behavior Encoder\n",
    "# -------------------------\n",
    "\n",
    "class BehaviorEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, tail2idx, device, debug=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.debug = debug\n",
    "        self.tail2idx = tail2idx\n",
    "\n",
    "        # === Base action vectors ===\n",
    "        self.e_clk = nn.Parameter(torch.tensor([1., 0., 0., 0.], device=device))\n",
    "        self.e_skp = nn.Parameter(torch.tensor([0., 1., 0., 0.], device=device))\n",
    "        self.e_gensumm = nn.Parameter(torch.tensor([0., 0., 1., 0.], device=device))\n",
    "        self.e_sumgen = nn.Parameter(torch.tensor([0., 0., 0., 1.], device=device))\n",
    "\n",
    "        # Action-specific transforms\n",
    "        self.W_clk = nn.Linear(4, hidden_dim, bias=False)\n",
    "        self.W_skp = nn.Linear(4, hidden_dim, bias=False)\n",
    "        self.W_gensumm = nn.Linear(4, hidden_dim, bias=False)\n",
    "        self.W_sumgen = nn.Linear(4, hidden_dim, bias=False)\n",
    "\n",
    "        # State transforms\n",
    "        self.W_pull = nn.Linear(1, hidden_dim, bias=False)\n",
    "        self.W_s = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_d = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "        # Fusion\n",
    "        self.Wh = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wc = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wz = nn.Linear(hidden_dim, 3, bias=False)\n",
    "        self.b_z = nn.Parameter(torch.zeros(3, device=device))\n",
    "        self.W_emb = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.b_emb = nn.Parameter(torch.zeros(hidden_dim, device=device))\n",
    "\n",
    "        # Rotation/translation\n",
    "        self.W_angle = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_theta = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        self.W_h = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_m = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "        # Scalars\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5, device=device))\n",
    "        self.beta = nn.Parameter(torch.tensor(0.5, device=device))\n",
    "\n",
    "        # Classifier head over tails\n",
    "        self.classifier = nn.Linear(hidden_dim, len(tail2idx))\n",
    "\n",
    "        # Next-step prediction head\n",
    "        self.W_next = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def _show(self, name, tensor, maxlen=6):\n",
    "        if not self.debug: \n",
    "            return\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            flat = tensor.detach().cpu().numpy().flatten()\n",
    "            vals = \", \".join(f\"{x:.4f}\" for x in flat[:maxlen])\n",
    "            if len(flat) > maxlen: vals += \", ...\"\n",
    "            print(f\"{name} (shape={tuple(tensor.shape)}): [{vals}]\")\n",
    "        else:\n",
    "            print(f\"{name}: {tensor}\")\n",
    "\n",
    "    def softmin_pool(self, a, b):\n",
    "        return -self.alpha * torch.log(torch.exp(a / self.alpha) +\n",
    "                                       torch.exp(b / self.alpha) + 1e-9)\n",
    "\n",
    "    def forward(self, Bhist, Bpos, lookup_df, tail2idx, embed_tables):\n",
    "        total_loss = torch.tensor(0., dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # === PASS 1: raw step embeddings (E_seq) ===\n",
    "        E_seq = []\n",
    "        h_clk = torch.zeros(self.hidden_dim, device=self.device)\n",
    "        h_skp = torch.zeros(self.hidden_dim, device=self.device)\n",
    "        h = torch.zeros(self.hidden_dim, device=self.device)\n",
    "\n",
    "        for t, b_id in enumerate(Bhist):\n",
    "            if b_id not in lookup_df.index:\n",
    "                continue\n",
    "            row = lookup_df.loc[b_id]\n",
    "            tail_id, rel = row['Tail'], row['Relation']\n",
    "\n",
    "            d_i = embed_tables['newsbody'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "            s_i = embed_tables['summary'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "            d_i_title = embed_tables['headline'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "\n",
    "            # init state\n",
    "            if t == 0:\n",
    "                head_emb = embed_tables['headline'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "                h_clk, h_skp = head_emb, head_emb\n",
    "                h = torch.sigmoid(self.W_s(head_emb)) * h_clk + (1 - torch.sigmoid(self.W_s(head_emb))) * h_skp\n",
    "\n",
    "            # relation-specific context c_i\n",
    "            if rel == \"click\":\n",
    "                c_i = (self.W_clk.weight @ self.e_clk * h) * d_i\n",
    "            elif rel == \"skip\":\n",
    "                d_ip1 = torch.zeros_like(d_i)\n",
    "                if t+1 < len(Bhist) and Bhist[t+1] in lookup_df.index:\n",
    "                    d_ip1 = embed_tables['newsbody'].get(\n",
    "                        lookup_df.loc[Bhist[t+1]]['Head'],\n",
    "                        torch.zeros_like(d_i)\n",
    "                    )\n",
    "                pull_term = self.W_pull(torch.tensor([[torch.dot(h_clk, d_ip1)+torch.dot(h_skp, d_i)]],\n",
    "                                                     device=self.device)).squeeze(0)\n",
    "                c_i = torch.tanh(self.W_skp.weight @ self.e_skp + d_i + pull_term) * h * d_i\n",
    "            elif rel == \"gen_summ\":\n",
    "                c_i = (self.W_gensumm.weight @ self.e_gensumm * h) * d_i_title\n",
    "            elif rel == \"summ_gen\":\n",
    "                gate_summgen = self.W_s(self.W_sumgen.weight @ self.e_sumgen)\n",
    "                c_i = self.softmin_pool(gate_summgen * s_i, (1 - gate_summgen) * d_i)\n",
    "                h_clk = h_clk + self.W_d((torch.ones_like(d_i_title) - d_i_title) * s_i)\n",
    "            else:\n",
    "                c_i = d_i\n",
    "\n",
    "            # update hidden\n",
    "            z_i = self.Wh(h) + self.Wc(c_i)\n",
    "            p_i = torch.softmax(self.Wz(z_i) + self.b_z, dim=-1)\n",
    "            m_i = p_i[0]*0.1 + p_i[1]*0.5 + p_i[2]*0.9\n",
    "            if rel == \"click\": h_clk = h_clk + m_i * c_i\n",
    "            elif rel == \"skip\": h_skp = h_skp * (1 - m_i) + c_i\n",
    "            h = self.beta * h_clk + (1 - self.beta) * h_skp\n",
    "\n",
    "            e_i = torch.tanh(self.W_emb(c_i) + self.b_emb)\n",
    "            E_seq.append(e_i)\n",
    "\n",
    "        if not E_seq:\n",
    "            return torch.zeros(self.hidden_dim, device=self.device), None, total_loss\n",
    "\n",
    "        # === PASS 2: contextualize (E‚Ä≤_seq) ===\n",
    "        Eprime_seq = []\n",
    "        eps = 1e-9\n",
    "        for i, e_i in enumerate(E_seq):\n",
    "            if i == 0:\n",
    "                e_prime = e_i\n",
    "            else:\n",
    "                e_prev, e_prime_prev = E_seq[i-1], Eprime_seq[-1]\n",
    "                theta_i = math.pi * torch.tanh(self.W_theta(torch.sigmoid(self.W_angle(e_prime_prev))))\n",
    "                m_i = F.softplus(self.W_m(self.W_h(e_prime_prev)))\n",
    "\n",
    "                v_i = (e_i - e_prime_prev) / (e_i - e_prime_prev).norm(p=2).clamp(min=eps)\n",
    "                u_prev = e_prev / e_prime_prev.norm(p=2).clamp(min=eps)\n",
    "                o_i = (v_i - torch.dot(v_i, u_prev) * u_prev)\n",
    "                o_i = o_i / o_i.norm(p=2).clamp(min=eps)\n",
    "\n",
    "                e_prime = e_prime_prev + m_i * (torch.cos(theta_i) * u_prev + torch.sin(theta_i) * o_i).squeeze()\n",
    "\n",
    "            Eprime_seq.append(e_prime)\n",
    "\n",
    "            # per-step auxiliary loss\n",
    "            tail_id = lookup_df.loc[Bhist[i]]['Tail']\n",
    "            if tail_id in self.tail2idx:\n",
    "                logits_step = self.classifier(e_prime.unsqueeze(0))\n",
    "                target_step = torch.tensor([self.tail2idx[tail_id]], device=self.device)\n",
    "                total_loss = total_loss + F.cross_entropy(logits_step, target_step)\n",
    "\n",
    "        # === Final prediction on Bpos ===\n",
    "        eprime_last = Eprime_seq[-1]\n",
    "\n",
    "        # Next-step embedding prediction\n",
    "        eprime_next = self.W_next(eprime_last)\n",
    "\n",
    "        logits_pos = None\n",
    "        if Bpos in lookup_df.index:\n",
    "            tail_id_pos = lookup_df.loc[Bpos]['Tail']\n",
    "            if tail_id_pos in self.tail2idx:\n",
    "                logits_pos = self.classifier(eprime_next.unsqueeze(0))\n",
    "                target_pos = torch.tensor([self.tail2idx[tail_id_pos]], device=self.device)\n",
    "                total_loss = total_loss + F.cross_entropy(logits_pos, target_pos)\n",
    "\n",
    "        return eprime_last, eprime_next, logits_pos, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aa1ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Inverse decoder for a single predicted embedding (d_next_pred as e'_pos)\n",
    "# -------------------------\n",
    "class BehaviorInverseDecoderPredict(nn.Module):\n",
    "    \"\"\"\n",
    "    Inverse mapping that takes a single predicted e' (embedding for Bpos)\n",
    "    and the head/headline embedding for Bpos, and returns:\n",
    "      - c'_pos (approx pseudo-content)\n",
    "      - s_hat_pos (approx summary)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, device, debug=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.debug = debug\n",
    "\n",
    "        # Learnable pseudo-inverse / disentanglers\n",
    "        self.W_emb_pinv = nn.Linear(hidden_dim, hidden_dim, bias=False)   # approx W_emb^+\n",
    "        self.W1_pinv = nn.Linear(hidden_dim, hidden_dim, bias=False)      # remove head contribution\n",
    "        self.W2_pinv = nn.Linear(hidden_dim, hidden_dim, bias=False)      # map residual -> summary\n",
    "\n",
    "    def _show(self, name, tensor, maxlen=6):\n",
    "        if not self.debug:\n",
    "            return\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            if tensor.ndim == 0:\n",
    "                print(f\"{name}: {tensor.item():.6f}\")\n",
    "            else:\n",
    "                flat = tensor.detach().cpu().numpy().flatten()\n",
    "                vals = \", \".join(f\"{x:.6f}\" for x in flat[:maxlen])\n",
    "                if len(flat) > maxlen: vals += \", ...\"\n",
    "                print(f\"{name} (shape={tuple(tensor.shape)}): [{vals}]\")\n",
    "        else:\n",
    "            print(f\"{name}: {tensor}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def atanh_safe(x, eps=1e-6):\n",
    "        x = x.clamp(-1+eps, 1-eps)\n",
    "        return 0.5 * torch.log((1+x) / (1-x))\n",
    "\n",
    "    def forward(self, eprime_pos, b_emb, h_pos):\n",
    "        \"\"\"\n",
    "        eprime_pos: tensor (hidden_dim,) -- predicted e' embedding for Bpos\n",
    "        b_emb: encoder bias b_emb (tensor (hidden_dim,))\n",
    "        h_pos: head/headline embedding for Bpos (tensor (hidden_dim,))\n",
    "        Returns: c_prime_pos, s_hat_pos\n",
    "        \"\"\"\n",
    "        # 1) invert embedding nonlinearity: atanh(e') - b_emb\n",
    "        x = self.atanh_safe(eprime_pos) - b_emb  # (hidden_dim,)\n",
    "        c_prime_pos = self.W_emb_pinv(x)         # approx c'_pos\n",
    "\n",
    "        # 2) subtract head contribution and map to summary\n",
    "        residual = c_prime_pos - self.W1_pinv(h_pos)\n",
    "        s_hat_pos = self.W2_pinv(residual)\n",
    "\n",
    "        # # debug prints\n",
    "        # self._show(\"eprime_pos\", eprime_pos)\n",
    "        # self._show(\"atanh(eprime_pos)-b_emb\", x)\n",
    "        # self._show(\"c'_pos\", c_prime_pos)\n",
    "        # self._show(\"residual (c' - W1^+ h)\", residual)\n",
    "        # self._show(\"s_hat_pos\", s_hat_pos)\n",
    "\n",
    "        return c_prime_pos, s_hat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e250e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonalizedT5Summarizer(nn.Module):\n",
    "    def __init__(self, hidden_dim, t5_model, behavior_encoder, inverse_decoder, device):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.t5 = t5_model.eval()\n",
    "        self.behavior_encoder = behavior_encoder\n",
    "        self.inverse_decoder = inverse_decoder\n",
    "        self.device = device\n",
    "\n",
    "        # Freeze T5\n",
    "        for param in self.t5.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Learnable gates and attention transforms\n",
    "        self.W_prime = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.W_qry = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.W_key = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.W_val = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, Bhist, Bpos, lookup_df, tail2idx, embed_tables, sid2sum, tokenizer, max_len=50):\n",
    "        # ----------------------\n",
    "        # 1Ô∏è‚É£ Behavior Encoder\n",
    "        # ----------------------\n",
    "        eprime_last, eprime_next, logits_pos, enc_loss = self.behavior_encoder(\n",
    "            Bhist, Bpos, lookup_df, tail2idx, embed_tables\n",
    "        )\n",
    "        # print(\"‚úÖ Behavior Encoder:\")\n",
    "        # print(f\"  eprime_last shape: {tuple(eprime_last.shape)}\")\n",
    "        # print(f\"  Encoder loss: {enc_loss.item():.4f}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # 2Ô∏è‚É£ Inverse Decoder ‚Üí s_hat\n",
    "        # ----------------------\n",
    "        head_emb = embed_tables['headline'].get(Bpos, torch.zeros(self.hidden_dim, device=self.device))\n",
    "        _, s_hat = self.inverse_decoder(eprime_last, self.behavior_encoder.b_emb, head_emb)\n",
    "        # print(\"‚úÖ Inverse Decoder:\")\n",
    "        # print(f\"  s_hat shape: {tuple(s_hat.shape)}\")\n",
    "        # print(f\"  s_hat sample (first 6 dims): {s_hat.detach().cpu().numpy()[:6]}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # 3Ô∏è‚É£ Personalized T5 Encoder\n",
    "        # ----------------------\n",
    "        if Bpos not in lookup_df.index:\n",
    "            #print(\"‚ö†Ô∏è Bpos not in lookup_df; returning encoder loss only.\")\n",
    "            return enc_loss, None\n",
    "\n",
    "        tail_id = lookup_df.loc[Bpos]['Tail']\n",
    "        doc_text = nid2body.get(tail_id, \" \")\n",
    "\n",
    "        input_text = \"generate headline for: \" + doc_text\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = self.t5.encoder(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        enc_states = encoder_outputs.last_hidden_state  # (1, seq_len, hidden_dim)\n",
    "        # print(\"‚úÖ Raw T5 Encoder States:\")\n",
    "        # print(f\"  Shape: {enc_states.shape}\")\n",
    "        # print(f\"  First token (first 10 dims): {enc_states[0,0,:10].detach().cpu().numpy()}\")\n",
    "\n",
    "        # Apply gated eprime_last\n",
    "        gated_eprime = torch.sigmoid(self.W_prime(eprime_last))  # (hidden_dim,)\n",
    "        enc_states_gated = enc_states * gated_eprime.unsqueeze(0).unsqueeze(0)\n",
    "        # print(\"‚úÖ Gated/Contextualized Encoder States:\")\n",
    "        # print(f\"  First token (first 10 dims): {enc_states_gated[0,0,:10].detach().cpu().numpy()}\")\n",
    "\n",
    "        # Inject s_hat\n",
    "        key = self.W_key(s_hat).unsqueeze(0).unsqueeze(1)   # (1,1,hidden_dim)\n",
    "        value = self.W_val(s_hat).unsqueeze(0).unsqueeze(1) # (1,1,hidden_dim)\n",
    "        seq_len = enc_states_gated.size(1)\n",
    "        key_exp = key.expand(-1, seq_len, -1)\n",
    "        value_exp = value.expand(-1, seq_len, -1)\n",
    "        personalized_enc_states = enc_states_gated + key_exp + value_exp\n",
    "        # print(\"‚úÖ Personalized Encoder States (after s_hat injection):\")\n",
    "        # print(f\"  First token (first 10 dims): {personalized_enc_states[0,0,:10].detach().cpu().numpy()}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # 4Ô∏è‚É£ Decoder with gold summary\n",
    "        # ----------------------\n",
    "        gold_summary_text = sid2sum.get(tail_id, \"\")\n",
    "        # print(\"Gold summary source:\",tail_id)\n",
    "        # print(\"Gold reference summaries:\",gold_summary_text)\n",
    "        # # if gold_summary_text == \"\":\n",
    "        #     print(\"‚ö†Ô∏è Gold summary missing; returning encoder loss only.\")\n",
    "        #     return enc_loss, None\n",
    "\n",
    "        decoder_inputs = tokenizer(\n",
    "            gold_summary_text, return_tensors=\"pt\", max_length=max_len, truncation=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # print(\"‚úÖ Target Summary Tokens:\")\n",
    "        # print(f\"  Token IDs: {decoder_inputs.input_ids[0].detach().cpu().numpy()[:min(10, decoder_inputs.input_ids.size(1))]}\")\n",
    "\n",
    "        outputs = self.t5(\n",
    "            input_ids=decoder_inputs.input_ids,\n",
    "            attention_mask=decoder_inputs.attention_mask,\n",
    "            encoder_outputs=(personalized_enc_states,),\n",
    "            labels=decoder_inputs.input_ids\n",
    "        )\n",
    "\n",
    "        pred_tokens = torch.argmax(outputs.logits, dim=-1)\n",
    "        # print(\"‚úÖ Predicted Summary Tokens:\")\n",
    "        # print(f\"  Token IDs: {pred_tokens[0,:min(10, pred_tokens.size(1))].detach().cpu().numpy()}\")\n",
    "\n",
    "        total_loss = enc_loss + outputs.loss\n",
    "        # print(\"‚úÖ Losses:\")\n",
    "        # print(f\"  Generation loss (T5): {outputs.loss.item():.4f}\")\n",
    "        # print(f\"  Behavior encoder loss: {enc_loss.item():.4f}\")\n",
    "        # print(f\"  Total loss: {total_loss.item():.4f}\")\n",
    "\n",
    "        return total_loss, outputs.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # 1Ô∏è‚É£ Sample one row\n",
    "# # -----------------------------\n",
    "# sample_row = train_df.sample(1).iloc[0]\n",
    "# Bhist = literal_eval(sample_row['EHist'])   # list of history Doc IDs\n",
    "# Bpos = sample_row['EPos']                # the current position doc ID (or use 'Bpos' column if present)\n",
    "# print(f\"üîπ Random Row UserID: {sample_row.UserID}\")\n",
    "# print(f\"üìù Sample Bhist: {Bhist}\")\n",
    "# print(f\"üéØ Target Bpos: {Bpos}\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # 2Ô∏è‚É£ Initialize models\n",
    "# # -----------------------------\n",
    "# behavior_encoder = BehaviorEncoder(hidden_dim, tail2idx, device, debug=True).to(device)\n",
    "# inverse_decoder = BehaviorInverseDecoderPredict(hidden_dim, device, debug=True).to(device)\n",
    "# personalized_model = PersonalizedT5Summarizer(hidden_dim, summarizer_model, behavior_encoder, inverse_decoder, device).to(device)\n",
    "\n",
    "# # -----------------------------\n",
    "# # 3Ô∏è‚É£ Forward pass through Behavior Encoder\n",
    "# # -----------------------------\n",
    "# eprime_last, eprime_next, logits_pos, enc_loss = behavior_encoder(\n",
    "#     Bhist, Bpos, lookup_df, tail2idx, embed_tables\n",
    "# )\n",
    "\n",
    "# print(\"\\n‚úÖ Behavior Encoder Output:\")\n",
    "# print(f\"  eprime_last shape: {eprime_last.shape}\")\n",
    "# print(f\"  eprime_next shape: {eprime_next.shape}\")\n",
    "# if logits_pos is not None:\n",
    "#     pred_tail_idx = torch.argmax(logits_pos, dim=-1).item()\n",
    "#     pred_tail = idx2tail[pred_tail_idx]\n",
    "#     print(f\"  Predicted tail from encoder: {pred_tail}\")\n",
    "# else:\n",
    "#     print(\"  No predicted tail (Bpos not in lookup)\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # 4Ô∏è‚É£ Forward pass through Inverse Decoder\n",
    "# # -----------------------------\n",
    "# head_emb = embed_tables['headline'].get(Bpos, torch.zeros(hidden_dim, device=device))\n",
    "# c_prime, s_hat = inverse_decoder(eprime_next, behavior_encoder.b_emb, head_emb)\n",
    "\n",
    "# print(\"\\n‚úÖ Inverse Decoder Output:\")\n",
    "# print(f\"  c'_pos (first 6 dims): {c_prime.detach().cpu().numpy()[:6]}\")\n",
    "# print(f\"  s_hat_pos (first 6 dims): {s_hat.detach().cpu().numpy()[:6]}\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # 5Ô∏è‚É£ Forward pass through Personalized T5\n",
    "# # -----------------------------\n",
    "# tail_id = lookup_df.loc[Bpos]['Tail']\n",
    "# print(\"Target summary id\",tail_id)\n",
    "# gold_summary_text = sid2sum.get(tail_id, \"\")\n",
    "\n",
    "# total_loss, logits = personalized_model(\n",
    "#     Bhist, Bpos, lookup_df, tail2idx, embed_tables, sid2sum, tokenizer, max_len=50\n",
    "# )\n",
    "\n",
    "# # -----------------------------\n",
    "# # 6Ô∏è‚É£ Decode predicted tokens\n",
    "# # -----------------------------\n",
    "# pred_summary = tokenizer.decode(torch.argmax(logits, dim=-1)[0], skip_special_tokens=True)\n",
    "# print(\"\\n‚úÖ Personalized T5 Output:\")\n",
    "# print(f\"  Target summary: {gold_summary_text[:300]}\")\n",
    "# print(f\"  Predicted summary: {pred_summary[:300]}\")\n",
    "# print(f\"  Total loss: {total_loss.item():.4f}\")\n",
    "# print(f\"  Encoder loss: {enc_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b453884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sid2sum.get(\"S-29150\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ea732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sid2sum.get(\"S-204452\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ba9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import torch\n",
    "# # import torch.nn as nn\n",
    "# # import torch.optim as optim\n",
    "# # from tqdm import tqdm\n",
    "# # import os\n",
    "\n",
    "# # -----------------------------\n",
    "# # Training setup\n",
    "# # -----------------------------\n",
    "# num_epochs = 3                # adjust as needed\n",
    "# batch_size = 1                # row-by-row training\n",
    "# learning_rate = 3e-4          # medium LR\n",
    "# save_path = \"walk2pers_checkpoint.pt\"\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     list(behavior_encoder.parameters()) +\n",
    "#     list(inverse_decoder.parameters()) +\n",
    "#     list(personalized_model.parameters()),\n",
    "#     lr=learning_rate\n",
    "# )\n",
    "\n",
    "# # make sure models are in train mode\n",
    "# behavior_encoder.train()\n",
    "# inverse_decoder.train()\n",
    "# personalized_model.train()\n",
    "\n",
    "# # pick first 100 rows\n",
    "# train_subset = train_df.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # -----------------------------\n",
    "# # Training Loop\n",
    "# # -----------------------------\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_loss = 0.0\n",
    "#     print(f\"\\nüîÑ Epoch {epoch+1}/{num_epochs} ---------------------------\")\n",
    "\n",
    "#     for i, row in tqdm(train_subset.iterrows(), total=len(train_subset), desc=f\"Epoch {epoch+1}\"):\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         Bhist = literal_eval(row['EHist'])\n",
    "#         Bpos = row['EPos']\n",
    "\n",
    "#         try:\n",
    "#             # forward pass through full model\n",
    "#             total_loss, logits = personalized_model(\n",
    "#                 Bhist, Bpos, lookup_df, tail2idx, embed_tables, sid2sum, tokenizer, max_len=50\n",
    "#             )\n",
    "\n",
    "#             # total loss weighting\n",
    "#             final_loss = 0.5 * total_loss   # already includes enc_loss + gen_loss\n",
    "#             # because total_loss was enc_loss + gen_loss\n",
    "#             # -> if you want strict 0.5*enc + 0.5*gen, separate them here\n",
    "\n",
    "#             final_loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             epoch_loss += final_loss.item()\n",
    "\n",
    "#             tqdm.write(f\"[Row {i}] Loss: {final_loss.item():.4f}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             tqdm.write(f\"[Row {i}] ‚ö†Ô∏è Skipped due to error: {str(e)}\")\n",
    "#             continue\n",
    "\n",
    "#     avg_loss = epoch_loss / len(train_subset)\n",
    "#     print(f\"‚úÖ Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#     # save model checkpoint\n",
    "#     torch.save({\n",
    "#         'epoch': epoch+1,\n",
    "#         'behavior_encoder_state': behavior_encoder.state_dict(),\n",
    "#         'inverse_decoder_state': inverse_decoder.state_dict(),\n",
    "#         'personalized_model_state': personalized_model.state_dict(),\n",
    "#         'optimizer_state': optimizer.state_dict(),\n",
    "#         'avg_loss': avg_loss,\n",
    "#     }, save_path)\n",
    "\n",
    "#     print(f\"üíæ Saved checkpoint to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2c7115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21756\\1742700192.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [04:14<00:00, 25.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID Query(Bpos.head)                                    Gold(Bpos.tail)  \\\n",
      "0  NT10_1          N101023  Behind the scenes photos of the British royal ...   \n",
      "1  NT10_2          N101705  Paul George to take on Kawhi Leonard's place w...   \n",
      "2  NT10_3          N121202          Societal view on the disabled having kids   \n",
      "\n",
      "                                 PredictedSummary         Loss      EncLoss  \n",
      "0     the wall, of the wall government family are  1720.188721  1715.235229  \n",
      "1  , his his hisftar.s  in his rest Al metro art.  1722.254150  1715.235229  \n",
      "2               etal and of the wall wall been in  1736.249878  1728.539185  \n",
      "\n",
      "Saved to inference_rows_101_110.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# -----------------------------\n",
    "# 0Ô∏è‚É£ Config & load checkpoint\n",
    "# -----------------------------\n",
    "hidden_dim = 768\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ckpt_path = \"walk2pers_checkpoint.pt\"\n",
    "\n",
    "# # fresh T5\n",
    "# t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "# for p in t5_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# re-instantiate wrappers\n",
    "behavior_encoder = BehaviorEncoder(hidden_dim, tail2idx, device).to(device)\n",
    "inverse_decoder = BehaviorInverseDecoderPredict(hidden_dim, device).to(device)\n",
    "personalized_model = PersonalizedT5Summarizer(hidden_dim, summarizer_model, behavior_encoder, inverse_decoder, device).to(device)\n",
    "\n",
    "# load checkpoint weights\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "behavior_encoder.load_state_dict(ckpt[\"behavior_encoder_state\"], strict=False)\n",
    "inverse_decoder.load_state_dict(ckpt[\"inverse_decoder_state\"], strict=False)\n",
    "personalized_model.load_state_dict(ckpt[\"personalized_model_state\"], strict=False)\n",
    "\n",
    "behavior_encoder.eval(); inverse_decoder.eval(); personalized_model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# 1Ô∏è‚É£ Slice rows 101‚Äì110\n",
    "# -----------------------------\n",
    "subset = test_df.iloc[200:210].reset_index(drop=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "# -----------------------------\n",
    "# 2Ô∏è‚É£ Loop with tqdm\n",
    "# -----------------------------\n",
    "for _, row in tqdm(subset.iterrows(), total=len(subset)):\n",
    "    try:\n",
    "        Bhist = literal_eval(row['EHist'])\n",
    "    except Exception:\n",
    "        Bhist = row['EHist']\n",
    "    Bpos  = row['EPos']\n",
    "    user  = row.get('UserID', None)\n",
    "\n",
    "    # Behavior Encoder\n",
    "    eprime_last, eprime_next, logits_pos, enc_loss = behavior_encoder(\n",
    "        Bhist, Bpos, lookup_df, tail2idx, embed_tables\n",
    "    )\n",
    "\n",
    "    # Inverse Decoder\n",
    "    head_emb = embed_tables['headline'].get(Bpos, torch.zeros(hidden_dim, device=device))\n",
    "    c_prime, s_hat = inverse_decoder(eprime_next, behavior_encoder.b_emb, head_emb)\n",
    "\n",
    "    # Gold summary (Bpos.tail)\n",
    "    tail_id = lookup_df.loc[Bpos]['Tail']\n",
    "    gold_summary_text = sid2sum.get(tail_id, \"\")\n",
    "\n",
    "    # Personalized T5 forward\n",
    "    total_loss, logits = personalized_model(\n",
    "        Bhist, Bpos, lookup_df, tail2idx, embed_tables, sid2sum, tokenizer, max_len=60\n",
    "    )\n",
    "\n",
    "    # Decode predicted tokens\n",
    "    pred_summary = tokenizer.decode(torch.argmax(logits, dim=-1)[0], skip_special_tokens=True)\n",
    "\n",
    "    # Query (Bpos.head body text)\n",
    "    head_id = lookup_df.loc[Bpos]['Head']\n",
    "    query_text = nid2body.get(head_id, \"\")\n",
    "\n",
    "    results.append({\n",
    "        \"UserID\": user,\n",
    "        \"Query(Bpos.head)\": head_id,\n",
    "        \"Gold(Bpos.tail)\": gold_summary_text,\n",
    "        \"PredictedSummary\": pred_summary,\n",
    "        \"Loss\": total_loss.item(),\n",
    "        \"EncLoss\": enc_loss.item() if enc_loss is not None else None\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# 3Ô∏è‚É£ Save + show results\n",
    "# -----------------------------\n",
    "df_out = pd.DataFrame(results)\n",
    "df_out.to_csv(\"inference_rows_101_110.csv\", index=False)\n",
    "\n",
    "print(df_out.head(3))\n",
    "print(\"\\nSaved to inference_rows_101_110.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2701689f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Query(Bpos.head)</th>\n",
       "      <th>Gold(Bpos.tail)</th>\n",
       "      <th>PredictedSummary</th>\n",
       "      <th>Loss</th>\n",
       "      <th>EncLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NT10_1</td>\n",
       "      <td>N101023</td>\n",
       "      <td>Behind the scenes photos of the British royal ...</td>\n",
       "      <td>the wall, of the wall government family are</td>\n",
       "      <td>1720.188721</td>\n",
       "      <td>1715.235229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NT10_2</td>\n",
       "      <td>N101705</td>\n",
       "      <td>Paul George to take on Kawhi Leonard's place w...</td>\n",
       "      <td>, his his hisftar.s  in his rest Al metro art.</td>\n",
       "      <td>1722.254150</td>\n",
       "      <td>1715.235229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NT10_3</td>\n",
       "      <td>N121202</td>\n",
       "      <td>Societal view on the disabled having kids</td>\n",
       "      <td>etal and of the wall wall been in</td>\n",
       "      <td>1736.249878</td>\n",
       "      <td>1728.539185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NT10_4</td>\n",
       "      <td>N11293</td>\n",
       "      <td>California leads in tackling CO2 emission by p...</td>\n",
       "      <td>' millions colora global2 in in thes millions ...</td>\n",
       "      <td>1737.121460</td>\n",
       "      <td>1728.539185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NT10_5</td>\n",
       "      <td>N65637</td>\n",
       "      <td>Catch of the day; full report</td>\n",
       "      <td>ing  day images - of</td>\n",
       "      <td>1735.705200</td>\n",
       "      <td>1728.539185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NT10_6</td>\n",
       "      <td>N114897</td>\n",
       "      <td>Target electricity outage caused uproar for fr...</td>\n",
       "      <td>e is of in by to in millionsaco relief in  awa...</td>\n",
       "      <td>1748.695312</td>\n",
       "      <td>1741.955078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NT10_7</td>\n",
       "      <td>N22881</td>\n",
       "      <td>Steven Duggar recovered but still a benchwarmer</td>\n",
       "      <td>Palglalace,  remains a fragment ining.</td>\n",
       "      <td>1764.114258</td>\n",
       "      <td>1755.435181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NT10_8</td>\n",
       "      <td>N40778</td>\n",
       "      <td>INSIDERS 's on celebrities drastic hairstyle t...</td>\n",
       "      <td>GraTA ‚ÄôONEs  wall' and colorss</td>\n",
       "      <td>1773.029663</td>\n",
       "      <td>1764.780884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NT10_9</td>\n",
       "      <td>N91777</td>\n",
       "      <td>Inside out on Montreal Canadiens drafting</td>\n",
       "      <td>r, the,aa dollars ador</td>\n",
       "      <td>1785.705566</td>\n",
       "      <td>1778.251343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NT10_10</td>\n",
       "      <td>N33507</td>\n",
       "      <td>Hall of Fame of this era's greatest athletes t...</td>\n",
       "      <td>e commerce painting Picasso colorera iss  pain...</td>\n",
       "      <td>1786.410645</td>\n",
       "      <td>1778.251343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    UserID Query(Bpos.head)  \\\n",
       "0   NT10_1          N101023   \n",
       "1   NT10_2          N101705   \n",
       "2   NT10_3          N121202   \n",
       "3   NT10_4           N11293   \n",
       "4   NT10_5           N65637   \n",
       "5   NT10_6          N114897   \n",
       "6   NT10_7           N22881   \n",
       "7   NT10_8           N40778   \n",
       "8   NT10_9           N91777   \n",
       "9  NT10_10           N33507   \n",
       "\n",
       "                                     Gold(Bpos.tail)  \\\n",
       "0  Behind the scenes photos of the British royal ...   \n",
       "1  Paul George to take on Kawhi Leonard's place w...   \n",
       "2          Societal view on the disabled having kids   \n",
       "3  California leads in tackling CO2 emission by p...   \n",
       "4                      Catch of the day; full report   \n",
       "5  Target electricity outage caused uproar for fr...   \n",
       "6    Steven Duggar recovered but still a benchwarmer   \n",
       "7  INSIDERS 's on celebrities drastic hairstyle t...   \n",
       "8          Inside out on Montreal Canadiens drafting   \n",
       "9  Hall of Fame of this era's greatest athletes t...   \n",
       "\n",
       "                                    PredictedSummary         Loss      EncLoss  \n",
       "0        the wall, of the wall government family are  1720.188721  1715.235229  \n",
       "1     , his his hisftar.s  in his rest Al metro art.  1722.254150  1715.235229  \n",
       "2                  etal and of the wall wall been in  1736.249878  1728.539185  \n",
       "3  ' millions colora global2 in in thes millions ...  1737.121460  1728.539185  \n",
       "4                               ing  day images - of  1735.705200  1728.539185  \n",
       "5  e is of in by to in millionsaco relief in  awa...  1748.695312  1741.955078  \n",
       "6             Palglalace,  remains a fragment ining.  1764.114258  1755.435181  \n",
       "7                     GraTA ‚ÄôONEs  wall' and colorss  1773.029663  1764.780884  \n",
       "8                             r, the,aa dollars ador  1785.705566  1778.251343  \n",
       "9  e commerce painting Picasso colorera iss  pain...  1786.410645  1778.251343  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
