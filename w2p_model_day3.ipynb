{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d29e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "import pickle\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4bcc80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Load T5-Base Model\n",
    "# ---------------------\n",
    "summarizer_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "summarizer_model.eval()\n",
    "for param in summarizer_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9be291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded nid2body with 113762 items\n",
      "üßæ Sample NID: N10000\n",
      "üìù Headline: Predicting Atlanta United's lineup against Columbus Crew in the U.S. Open Cup\n"
     ]
    }
   ],
   "source": [
    "# Load nid2body from pickle\n",
    "with open(\"nid2body.pkl\", \"rb\") as f:\n",
    "    nid2body = pickle.load(f)\n",
    "\n",
    "# Debug print\n",
    "print(f\"‚úÖ Loaded nid2body with {len(nid2body)} items\")\n",
    "sample_nid = list(nid2body.keys())[0]\n",
    "print(f\"üßæ Sample NID: {sample_nid}\\nüìù Headline: {nid2body[sample_nid][:300]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08fb8c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded sid2sum with 135001 items\n",
      "üßæ Sample SID: S-1\n",
      "üìù Summary: The officer reportedly also pointed his gun at Harper and her children.\n"
     ]
    }
   ],
   "source": [
    "# Load sid2sum from pickle\n",
    "with open(\"sid2sum.pkl\", \"rb\") as f:\n",
    "    sid2sum = pickle.load(f)\n",
    "\n",
    "# Debug print\n",
    "print(f\"‚úÖ Loaded sid2sum with {len(sid2sum)} items\")\n",
    "sample_sid = list(sid2sum.keys())[0]\n",
    "print(f\"üßæ Sample SID: {sample_sid}\\nüìù Summary: {sid2sum[sample_sid][:300]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e60dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Device and Precision Setup ===\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_dim = 768\n",
    "\n",
    "# === Utility Functions ===\n",
    "def get_embedding(key, table, dim):\n",
    "    if key not in table:\n",
    "        table[key] = torch.nn.Parameter(torch.randn(dim, dtype=torch.float32, device=device) * 0.01, requires_grad=True)\n",
    "    return table[key]\n",
    "\n",
    "\n",
    "# === Load Embeddings ===\n",
    "with open(\"summary_T5.pkl\", \"rb\") as f:\n",
    "    summary_embed = {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in pickle.load(f).items()}\n",
    "with open(\"newsbody_T5.pkl\", \"rb\") as f:\n",
    "    newsbody_embed = {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in pickle.load(f).items()}\n",
    "with open(\"headline_T5.pkl\", \"rb\") as f:\n",
    "    headline_embed = {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in pickle.load(f).items()}\n",
    "\n",
    "embed_tables = {\n",
    "    'summary': summary_embed,\n",
    "    'newsbody': newsbody_embed,\n",
    "    'headline': headline_embed\n",
    "}\n",
    "\n",
    "# === Load Dataset ===\n",
    "lookup_df = pd.read_csv(\"w2p_engage_list.csv\").set_index('EdgeID')\n",
    "train_df = pd.read_csv(\"train_w2p.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "083823d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UserID</th>\n",
       "      <th>EHist</th>\n",
       "      <th>EPos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>U10000_1</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>U10000_2</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>U100006_1</td>\n",
       "      <td>['E151']</td>\n",
       "      <td>E152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>U100006_2</td>\n",
       "      <td>['E151', 'E152', 'E153', 'E154', 'E155', 'E156...</td>\n",
       "      <td>E168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>U100006_3</td>\n",
       "      <td>['E151', 'E152', 'E153', 'E154', 'E155', 'E156...</td>\n",
       "      <td>E230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>195583</td>\n",
       "      <td>U233775_3</td>\n",
       "      <td>['E1882808', 'E1882809', 'E1882810', 'E1882811...</td>\n",
       "      <td>E1882859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>195584</td>\n",
       "      <td>U233775_4</td>\n",
       "      <td>['E1882808', 'E1882809', 'E1882810', 'E1882811...</td>\n",
       "      <td>E1882866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>195585</td>\n",
       "      <td>U233775_5</td>\n",
       "      <td>['E1882808', 'E1882809', 'E1882810', 'E1882811...</td>\n",
       "      <td>E1882869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>195586</td>\n",
       "      <td>U233775_6</td>\n",
       "      <td>['E1882808', 'E1882809', 'E1882810', 'E1882811...</td>\n",
       "      <td>E1882920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>195587</td>\n",
       "      <td>U233777_1</td>\n",
       "      <td>['E1882958', 'E1882959', 'E1882960', 'E1882961...</td>\n",
       "      <td>E1883030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     UserID  \\\n",
       "0               2   U10000_1   \n",
       "1               3   U10000_2   \n",
       "2              11  U100006_1   \n",
       "3              12  U100006_2   \n",
       "4              13  U100006_3   \n",
       "...           ...        ...   \n",
       "49995      195583  U233775_3   \n",
       "49996      195584  U233775_4   \n",
       "49997      195585  U233775_5   \n",
       "49998      195586  U233775_6   \n",
       "49999      195587  U233777_1   \n",
       "\n",
       "                                                   EHist      EPos  \n",
       "0      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...       E84  \n",
       "1      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...      E133  \n",
       "2                                               ['E151']      E152  \n",
       "3      ['E151', 'E152', 'E153', 'E154', 'E155', 'E156...      E168  \n",
       "4      ['E151', 'E152', 'E153', 'E154', 'E155', 'E156...      E230  \n",
       "...                                                  ...       ...  \n",
       "49995  ['E1882808', 'E1882809', 'E1882810', 'E1882811...  E1882859  \n",
       "49996  ['E1882808', 'E1882809', 'E1882810', 'E1882811...  E1882866  \n",
       "49997  ['E1882808', 'E1882809', 'E1882810', 'E1882811...  E1882869  \n",
       "49998  ['E1882808', 'E1882809', 'E1882810', 'E1882811...  E1882920  \n",
       "49999  ['E1882958', 'E1882959', 'E1882960', 'E1882961...  E1883030  \n",
       "\n",
       "[50000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=train_df[:50000]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe86336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Head</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Tail</th>\n",
       "      <th>User</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EdgeID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E1</th>\n",
       "      <td>0</td>\n",
       "      <td>U10000</td>\n",
       "      <td>skip</td>\n",
       "      <td>N110699</td>\n",
       "      <td>U10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E2</th>\n",
       "      <td>1</td>\n",
       "      <td>N110699</td>\n",
       "      <td>skip</td>\n",
       "      <td>N104733</td>\n",
       "      <td>U10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E3</th>\n",
       "      <td>2</td>\n",
       "      <td>N104733</td>\n",
       "      <td>skip</td>\n",
       "      <td>N80645</td>\n",
       "      <td>U10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E4</th>\n",
       "      <td>3</td>\n",
       "      <td>N80645</td>\n",
       "      <td>skip</td>\n",
       "      <td>N76869</td>\n",
       "      <td>U10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E5</th>\n",
       "      <td>4</td>\n",
       "      <td>N76869</td>\n",
       "      <td>skip</td>\n",
       "      <td>N119531</td>\n",
       "      <td>U10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E18413173</th>\n",
       "      <td>18413172</td>\n",
       "      <td>N107780</td>\n",
       "      <td>click</td>\n",
       "      <td>N97051</td>\n",
       "      <td>U249644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E18413174</th>\n",
       "      <td>18413173</td>\n",
       "      <td>N97051</td>\n",
       "      <td>click</td>\n",
       "      <td>N81002</td>\n",
       "      <td>U249644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E18413175</th>\n",
       "      <td>18413174</td>\n",
       "      <td>N81002</td>\n",
       "      <td>click</td>\n",
       "      <td>N11725</td>\n",
       "      <td>U249644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E18413176</th>\n",
       "      <td>18413175</td>\n",
       "      <td>N11725</td>\n",
       "      <td>click</td>\n",
       "      <td>N89229</td>\n",
       "      <td>U249644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E18413177</th>\n",
       "      <td>18413176</td>\n",
       "      <td>N89229</td>\n",
       "      <td>skip</td>\n",
       "      <td>N67207</td>\n",
       "      <td>U249644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18413177 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Unnamed: 0     Head Relation     Tail     User\n",
       "EdgeID                                                   \n",
       "E1                  0   U10000     skip  N110699   U10000\n",
       "E2                  1  N110699     skip  N104733   U10000\n",
       "E3                  2  N104733     skip   N80645   U10000\n",
       "E4                  3   N80645     skip   N76869   U10000\n",
       "E5                  4   N76869     skip  N119531   U10000\n",
       "...               ...      ...      ...      ...      ...\n",
       "E18413173    18413172  N107780    click   N97051  U249644\n",
       "E18413174    18413173   N97051    click   N81002  U249644\n",
       "E18413175    18413174   N81002    click   N11725  U249644\n",
       "E18413176    18413175   N11725    click   N89229  U249644\n",
       "E18413177    18413176   N89229     skip   N67207  U249644\n",
       "\n",
       "[18413177 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43956d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------\n",
    "# # Build Tail2Idx\n",
    "# # ---------------------\n",
    "# tail_set = set()\n",
    "\n",
    "# print(\"Building Tail2Idx from EHist and EPos...\")\n",
    "# for row in tqdm(train_df.itertuples(), total=len(train_df), desc=\"Collecting tails\"):\n",
    "#     try:\n",
    "#         bhist = literal_eval(row.EHist)\n",
    "#         bpos = row.EPos\n",
    "\n",
    "#         # Add tails from Bhist\n",
    "#         for b_id in bhist:\n",
    "#             if b_id in lookup_df.index:\n",
    "#                 tail = lookup_df.loc[b_id, 'Tail']\n",
    "#                 tail_set.add(tail)\n",
    "\n",
    "#         # Add tail from Bpos\n",
    "#         if bpos in lookup_df.index:\n",
    "#             tail = lookup_df.loc[bpos, 'Tail']\n",
    "#             tail_set.add(tail)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"[Skip] Error in row {row.Index}: {e}\")\n",
    "\n",
    "# # === Final mappings ===\n",
    "# tail_ids = sorted(tail_set)\n",
    "# tail2idx = {tid: idx for idx, tid in enumerate(tail_ids)}\n",
    "# idx2tail = {idx: tid for tid, idx in tail2idx.items()}\n",
    "\n",
    "# print(f\"‚úÖ Tail2Idx built with {len(tail2idx)} unique tail IDs.\")\n",
    "\n",
    "# # ---------------------\n",
    "# # Save as pickle\n",
    "# # ---------------------\n",
    "# with open(\"tail_mappings.pkl\", \"wb\") as f:\n",
    "#     pickle.dump({\"tail2idx\": tail2idx, \"idx2tail\": idx2tail}, f)\n",
    "\n",
    "# print(\"üíæ Saved tail2idx and idx2tail to tail_mappings.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca969e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded tail2idx with 544137 entries\n",
      "üßæ Sample tail ‚Üí idx: [('N10000', 0), ('N100001', 1), ('N100003', 2)]\n",
      "üßæ Sample idx ‚Üí tail: [(0, 'N10000'), (1, 'N100001'), (2, 'N100003')]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Load back from pickle\n",
    "# ---------------------\n",
    "with open(\"tail_mappings.pkl\", \"rb\") as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "tail2idx = mappings[\"tail2idx\"]\n",
    "idx2tail = mappings[\"idx2tail\"]\n",
    "\n",
    "print(f\"‚úÖ Loaded tail2idx with {len(tail2idx)} entries\")\n",
    "print(f\"üßæ Sample tail ‚Üí idx: {list(tail2idx.items())[:3]}\")\n",
    "print(f\"üßæ Sample idx ‚Üí tail: {list(idx2tail.items())[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5378608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Sequence Engagement/Behavior Encoder\n",
    "# -------------------------\n",
    "\n",
    "class BehaviorEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, tail2idx, device, debug=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.debug = debug\n",
    "        self.tail2idx = tail2idx\n",
    "\n",
    "        # === Base action vectors ===\n",
    "        self.e_clk = nn.Parameter(torch.tensor([1., 0., 0., 0.], device=device))\n",
    "        self.e_skp = nn.Parameter(torch.tensor([0., 1., 0., 0.], device=device))\n",
    "        self.e_gensumm = nn.Parameter(torch.tensor([0., 0., 1., 0.], device=device))\n",
    "        self.e_sumgen = nn.Parameter(torch.tensor([0., 0., 0., 1.], device=device))\n",
    "\n",
    "        # Action-specific transforms\n",
    "        self.W_clk = nn.Linear(4, hidden_dim, bias=False)\n",
    "        self.W_skp = nn.Linear(4, hidden_dim, bias=False)\n",
    "        self.W_gensumm = nn.Linear(4, hidden_dim, bias=False)\n",
    "        self.W_sumgen = nn.Linear(4, hidden_dim, bias=False)\n",
    "\n",
    "        # State transforms\n",
    "        self.W_pull = nn.Linear(1, hidden_dim, bias=False)\n",
    "        self.W_s = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_d = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "        # Fusion\n",
    "        self.Wh = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wc = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wz = nn.Linear(hidden_dim, 3, bias=False)\n",
    "        self.b_z = nn.Parameter(torch.zeros(3, device=device))\n",
    "        self.W_emb = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.b_emb = nn.Parameter(torch.zeros(hidden_dim, device=device))\n",
    "\n",
    "        # Rotation/translation\n",
    "        self.W_angle = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_theta = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        self.W_h = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_m = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "        # Scalars\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5, device=device))\n",
    "        self.beta = nn.Parameter(torch.tensor(0.5, device=device))\n",
    "\n",
    "        # Classifier head over tails\n",
    "        self.classifier = nn.Linear(hidden_dim, len(tail2idx))\n",
    "\n",
    "        # Next-step prediction head\n",
    "        self.W_next = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def _show(self, name, tensor, maxlen=6):\n",
    "        if not self.debug: \n",
    "            return\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            flat = tensor.detach().cpu().numpy().flatten()\n",
    "            vals = \", \".join(f\"{x:.4f}\" for x in flat[:maxlen])\n",
    "            if len(flat) > maxlen: vals += \", ...\"\n",
    "            print(f\"{name} (shape={tuple(tensor.shape)}): [{vals}]\")\n",
    "        else:\n",
    "            print(f\"{name}: {tensor}\")\n",
    "\n",
    "    def softmin_pool(self, a, b):\n",
    "        return -self.alpha * torch.log(torch.exp(a / self.alpha) +\n",
    "                                       torch.exp(b / self.alpha) + 1e-9)\n",
    "\n",
    "    def forward(self, Bhist, Bpos, lookup_df, tail2idx, embed_tables):\n",
    "        total_loss = torch.tensor(0., dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # === PASS 1: raw step embeddings (E_seq) ===\n",
    "        E_seq = []\n",
    "        h_clk = torch.zeros(self.hidden_dim, device=self.device)\n",
    "        h_skp = torch.zeros(self.hidden_dim, device=self.device)\n",
    "        h = torch.zeros(self.hidden_dim, device=self.device)\n",
    "\n",
    "        for t, b_id in enumerate(Bhist):\n",
    "            if b_id not in lookup_df.index:\n",
    "                continue\n",
    "            row = lookup_df.loc[b_id]\n",
    "            tail_id, rel = row['Tail'], row['Relation']\n",
    "\n",
    "            d_i = embed_tables['newsbody'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "            s_i = embed_tables['summary'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "            d_i_title = embed_tables['headline'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "\n",
    "            # init state\n",
    "            if t == 0:\n",
    "                head_emb = embed_tables['headline'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "                h_clk, h_skp = head_emb, head_emb\n",
    "                h = torch.sigmoid(self.W_s(head_emb)) * h_clk + (1 - torch.sigmoid(self.W_s(head_emb))) * h_skp\n",
    "\n",
    "            # relation-specific context c_i\n",
    "            if rel == \"click\":\n",
    "                c_i = (self.W_clk.weight @ self.e_clk * h) * d_i\n",
    "            elif rel == \"skip\":\n",
    "                d_ip1 = torch.zeros_like(d_i)\n",
    "                if t+1 < len(Bhist) and Bhist[t+1] in lookup_df.index:\n",
    "                    d_ip1 = embed_tables['newsbody'].get(\n",
    "                        lookup_df.loc[Bhist[t+1]]['Head'],\n",
    "                        torch.zeros_like(d_i)\n",
    "                    )\n",
    "                pull_term = self.W_pull(torch.tensor([[torch.dot(h_clk, d_ip1)+torch.dot(h_skp, d_i)]],\n",
    "                                                     device=self.device)).squeeze(0)\n",
    "                c_i = torch.tanh(self.W_skp.weight @ self.e_skp + d_i + pull_term) * h * d_i\n",
    "            elif rel == \"gen_summ\":\n",
    "                c_i = (self.W_gensumm.weight @ self.e_gensumm * h) * d_i_title\n",
    "            elif rel == \"summ_gen\":\n",
    "                gate_summgen = self.W_s(self.W_sumgen.weight @ self.e_sumgen)\n",
    "                c_i = self.softmin_pool(gate_summgen * s_i, (1 - gate_summgen) * d_i)\n",
    "                h_clk = h_clk + self.W_d((torch.ones_like(d_i_title) - d_i_title) * s_i)\n",
    "            else:\n",
    "                c_i = d_i\n",
    "\n",
    "            # update hidden\n",
    "            z_i = self.Wh(h) + self.Wc(c_i)\n",
    "            p_i = torch.softmax(self.Wz(z_i) + self.b_z, dim=-1)\n",
    "            m_i = p_i[0]*0.1 + p_i[1]*0.5 + p_i[2]*0.9\n",
    "            if rel == \"click\": h_clk = h_clk + m_i * c_i\n",
    "            elif rel == \"skip\": h_skp = h_skp * (1 - m_i) + c_i\n",
    "            h = self.beta * h_clk + (1 - self.beta) * h_skp\n",
    "\n",
    "            e_i = torch.tanh(self.W_emb(c_i) + self.b_emb)\n",
    "            E_seq.append(e_i)\n",
    "\n",
    "        if not E_seq:\n",
    "            return torch.zeros(self.hidden_dim, device=self.device), None, total_loss\n",
    "\n",
    "        # === PASS 2: contextualize (E‚Ä≤_seq) ===\n",
    "        Eprime_seq = []\n",
    "        eps = 1e-9\n",
    "        for i, e_i in enumerate(E_seq):\n",
    "            if i == 0:\n",
    "                e_prime = e_i\n",
    "            else:\n",
    "                e_prev, e_prime_prev = E_seq[i-1], Eprime_seq[-1]\n",
    "                theta_i = math.pi * torch.tanh(self.W_theta(torch.sigmoid(self.W_angle(e_prime_prev))))\n",
    "                m_i = F.softplus(self.W_m(self.W_h(e_prime_prev)))\n",
    "\n",
    "                v_i = (e_i - e_prime_prev) / (e_i - e_prime_prev).norm(p=2).clamp(min=eps)\n",
    "                u_prev = e_prev / e_prime_prev.norm(p=2).clamp(min=eps)\n",
    "                o_i = (v_i - torch.dot(v_i, u_prev) * u_prev)\n",
    "                o_i = o_i / o_i.norm(p=2).clamp(min=eps)\n",
    "\n",
    "                e_prime = e_prime_prev + m_i * (torch.cos(theta_i) * u_prev + torch.sin(theta_i) * o_i).squeeze()\n",
    "\n",
    "            Eprime_seq.append(e_prime)\n",
    "\n",
    "            # per-step auxiliary loss\n",
    "            tail_id = lookup_df.loc[Bhist[i]]['Tail']\n",
    "            if tail_id in self.tail2idx:\n",
    "                logits_step = self.classifier(e_prime.unsqueeze(0))\n",
    "                target_step = torch.tensor([self.tail2idx[tail_id]], device=self.device)\n",
    "                total_loss = total_loss + F.cross_entropy(logits_step, target_step)\n",
    "\n",
    "        # === Final prediction on Bpos ===\n",
    "        eprime_last = Eprime_seq[-1]\n",
    "\n",
    "        # Next-step embedding prediction\n",
    "        eprime_next = self.W_next(eprime_last)\n",
    "\n",
    "        logits_pos = None\n",
    "        if Bpos in lookup_df.index:\n",
    "            tail_id_pos = lookup_df.loc[Bpos]['Tail']\n",
    "            if tail_id_pos in self.tail2idx:\n",
    "                logits_pos = self.classifier(eprime_next.unsqueeze(0))\n",
    "                target_pos = torch.tensor([self.tail2idx[tail_id_pos]], device=self.device)\n",
    "                total_loss = total_loss + F.cross_entropy(logits_pos, target_pos)\n",
    "\n",
    "        return eprime_last, eprime_next, logits_pos, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aa1ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Inverse decoder for a single predicted embedding (d_next_pred as e'_pos)\n",
    "# -------------------------\n",
    "class BehaviorInverseDecoderPredict(nn.Module):\n",
    "    \"\"\"\n",
    "    Inverse mapping that takes a single predicted e' (embedding for Bpos)\n",
    "    and the head/headline embedding for Bpos, and returns:\n",
    "      - c'_pos (approx pseudo-content)\n",
    "      - s_hat_pos (approx summary)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, device, debug=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.debug = debug\n",
    "\n",
    "        # Learnable pseudo-inverse / disentanglers\n",
    "        self.W_emb_pinv = nn.Linear(hidden_dim, hidden_dim, bias=False)   # approx W_emb^+\n",
    "        self.W1_pinv = nn.Linear(hidden_dim, hidden_dim, bias=False)      # remove head contribution\n",
    "        self.W2_pinv = nn.Linear(hidden_dim, hidden_dim, bias=False)      # map residual -> summary\n",
    "\n",
    "    def _show(self, name, tensor, maxlen=6):\n",
    "        if not self.debug:\n",
    "            return\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            if tensor.ndim == 0:\n",
    "                print(f\"{name}: {tensor.item():.6f}\")\n",
    "            else:\n",
    "                flat = tensor.detach().cpu().numpy().flatten()\n",
    "                vals = \", \".join(f\"{x:.6f}\" for x in flat[:maxlen])\n",
    "                if len(flat) > maxlen: vals += \", ...\"\n",
    "                print(f\"{name} (shape={tuple(tensor.shape)}): [{vals}]\")\n",
    "        else:\n",
    "            print(f\"{name}: {tensor}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def atanh_safe(x, eps=1e-6):\n",
    "        x = x.clamp(-1+eps, 1-eps)\n",
    "        return 0.5 * torch.log((1+x) / (1-x))\n",
    "\n",
    "    def forward(self, eprime_pos, b_emb, h_pos):\n",
    "        \"\"\"\n",
    "        eprime_pos: tensor (hidden_dim,) -- predicted e' embedding for Bpos\n",
    "        b_emb: encoder bias b_emb (tensor (hidden_dim,))\n",
    "        h_pos: head/headline embedding for Bpos (tensor (hidden_dim,))\n",
    "        Returns: c_prime_pos, s_hat_pos\n",
    "        \"\"\"\n",
    "        # 1) invert embedding nonlinearity: atanh(e') - b_emb\n",
    "        x = self.atanh_safe(eprime_pos) - b_emb  # (hidden_dim,)\n",
    "        c_prime_pos = self.W_emb_pinv(x)         # approx c'_pos\n",
    "\n",
    "        # 2) subtract head contribution and map to summary\n",
    "        residual = c_prime_pos - self.W1_pinv(h_pos)\n",
    "        s_hat_pos = self.W2_pinv(residual)\n",
    "\n",
    "        # # debug prints\n",
    "        # self._show(\"eprime_pos\", eprime_pos)\n",
    "        # self._show(\"atanh(eprime_pos)-b_emb\", x)\n",
    "        # self._show(\"c'_pos\", c_prime_pos)\n",
    "        # self._show(\"residual (c' - W1^+ h)\", residual)\n",
    "        # self._show(\"s_hat_pos\", s_hat_pos)\n",
    "\n",
    "        return c_prime_pos, s_hat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e250e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonalizedT5Summarizer(nn.Module):\n",
    "    def __init__(self, hidden_dim, t5_model, behavior_encoder, inverse_decoder, device):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.t5 = t5_model.eval()\n",
    "        self.behavior_encoder = behavior_encoder\n",
    "        self.inverse_decoder = inverse_decoder\n",
    "        self.device = device\n",
    "\n",
    "        # Freeze T5\n",
    "        for param in self.t5.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Learnable gates and attention transforms\n",
    "        self.W_prime = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.W_qry = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.W_key = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.W_val = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, Bhist, Bpos, lookup_df, tail2idx, embed_tables, sid2sum, tokenizer, max_len=50):\n",
    "        # ----------------------\n",
    "        # 1Ô∏è‚É£ Behavior Encoder\n",
    "        # ----------------------\n",
    "        eprime_last, eprime_next, logits_pos, enc_loss = self.behavior_encoder(\n",
    "            Bhist, Bpos, lookup_df, tail2idx, embed_tables\n",
    "        )\n",
    "        # print(\"‚úÖ Behavior Encoder:\")\n",
    "        # print(f\"  eprime_last shape: {tuple(eprime_last.shape)}\")\n",
    "        # print(f\"  Encoder loss: {enc_loss.item():.4f}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # 2Ô∏è‚É£ Inverse Decoder ‚Üí s_hat\n",
    "        # ----------------------\n",
    "        head_emb = embed_tables['headline'].get(Bpos, torch.zeros(self.hidden_dim, device=self.device))\n",
    "        _, s_hat = self.inverse_decoder(eprime_last, self.behavior_encoder.b_emb, head_emb)\n",
    "        # print(\"‚úÖ Inverse Decoder:\")\n",
    "        # print(f\"  s_hat shape: {tuple(s_hat.shape)}\")\n",
    "        # print(f\"  s_hat sample (first 6 dims): {s_hat.detach().cpu().numpy()[:6]}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # 3Ô∏è‚É£ Personalized T5 Encoder\n",
    "        # ----------------------\n",
    "        if Bpos not in lookup_df.index:\n",
    "            #print(\"‚ö†Ô∏è Bpos not in lookup_df; returning encoder loss only.\")\n",
    "            return enc_loss, None\n",
    "\n",
    "        tail_id = lookup_df.loc[Bpos]['Tail']\n",
    "        doc_text = nid2body.get(tail_id, \" \")\n",
    "\n",
    "        input_text = \"generate headline for: \" + doc_text\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = self.t5.encoder(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        enc_states = encoder_outputs.last_hidden_state  # (1, seq_len, hidden_dim)\n",
    "        # print(\"‚úÖ Raw T5 Encoder States:\")\n",
    "        # print(f\"  Shape: {enc_states.shape}\")\n",
    "        # print(f\"  First token (first 10 dims): {enc_states[0,0,:10].detach().cpu().numpy()}\")\n",
    "\n",
    "        # Apply gated eprime_last\n",
    "        gated_eprime = torch.sigmoid(self.W_prime(eprime_last))  # (hidden_dim,)\n",
    "        enc_states_gated = enc_states * gated_eprime.unsqueeze(0).unsqueeze(0)\n",
    "        # print(\"‚úÖ Gated/Contextualized Encoder States:\")\n",
    "        # print(f\"  First token (first 10 dims): {enc_states_gated[0,0,:10].detach().cpu().numpy()}\")\n",
    "\n",
    "        # Inject s_hat\n",
    "        key = self.W_key(s_hat).unsqueeze(0).unsqueeze(1)   # (1,1,hidden_dim)\n",
    "        value = self.W_val(s_hat).unsqueeze(0).unsqueeze(1) # (1,1,hidden_dim)\n",
    "        seq_len = enc_states_gated.size(1)\n",
    "        key_exp = key.expand(-1, seq_len, -1)\n",
    "        value_exp = value.expand(-1, seq_len, -1)\n",
    "        personalized_enc_states = enc_states_gated + key_exp + value_exp\n",
    "        # print(\"‚úÖ Personalized Encoder States (after s_hat injection):\")\n",
    "        # print(f\"  First token (first 10 dims): {personalized_enc_states[0,0,:10].detach().cpu().numpy()}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # 4Ô∏è‚É£ Decoder with gold summary\n",
    "        # ----------------------\n",
    "        gold_summary_text = sid2sum.get(tail_id, \"\")\n",
    "        # print(\"Gold summary source:\",tail_id)\n",
    "        # print(\"Gold reference summaries:\",gold_summary_text)\n",
    "        # # if gold_summary_text == \"\":\n",
    "        #     print(\"‚ö†Ô∏è Gold summary missing; returning encoder loss only.\")\n",
    "        #     return enc_loss, None\n",
    "\n",
    "        decoder_inputs = tokenizer(\n",
    "            gold_summary_text, return_tensors=\"pt\", max_length=max_len, truncation=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # print(\"‚úÖ Target Summary Tokens:\")\n",
    "        # print(f\"  Token IDs: {decoder_inputs.input_ids[0].detach().cpu().numpy()[:min(10, decoder_inputs.input_ids.size(1))]}\")\n",
    "\n",
    "        outputs = self.t5(\n",
    "            input_ids=decoder_inputs.input_ids,\n",
    "            attention_mask=decoder_inputs.attention_mask,\n",
    "            encoder_outputs=(personalized_enc_states,),\n",
    "            labels=decoder_inputs.input_ids\n",
    "        )\n",
    "\n",
    "        pred_tokens = torch.argmax(outputs.logits, dim=-1)\n",
    "        # print(\"‚úÖ Predicted Summary Tokens:\")\n",
    "        # print(f\"  Token IDs: {pred_tokens[0,:min(10, pred_tokens.size(1))].detach().cpu().numpy()}\")\n",
    "\n",
    "        total_loss = enc_loss + outputs.loss\n",
    "        # print(\"‚úÖ Losses:\")\n",
    "        # print(f\"  Generation loss (T5): {outputs.loss.item():.4f}\")\n",
    "        # print(f\"  Behavior encoder loss: {enc_loss.item():.4f}\")\n",
    "        # print(f\"  Total loss: {total_loss.item():.4f}\")\n",
    "\n",
    "        return total_loss, outputs.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1Ô∏è‚É£ Sample one row\n",
    "# -----------------------------\n",
    "sample_row = train_df.sample(1).iloc[0]\n",
    "Bhist = literal_eval(sample_row['EHist'])   # list of history Doc IDs\n",
    "Bpos = sample_row['EPos']                # the current position doc ID (or use 'Bpos' column if present)\n",
    "print(f\"üîπ Random Row UserID: {sample_row.UserID}\")\n",
    "print(f\"üìù Sample Bhist: {Bhist}\")\n",
    "print(f\"üéØ Target Bpos: {Bpos}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2Ô∏è‚É£ Initialize models\n",
    "# -----------------------------\n",
    "behavior_encoder = BehaviorEncoder(hidden_dim, tail2idx, device, debug=True).to(device)\n",
    "inverse_decoder = BehaviorInverseDecoderPredict(hidden_dim, device, debug=True).to(device)\n",
    "personalized_model = PersonalizedT5Summarizer(hidden_dim, summarizer_model, behavior_encoder, inverse_decoder, device).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 3Ô∏è‚É£ Forward pass through Behavior Encoder\n",
    "# -----------------------------\n",
    "eprime_last, eprime_next, logits_pos, enc_loss = behavior_encoder(\n",
    "    Bhist, Bpos, lookup_df, tail2idx, embed_tables\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Behavior Encoder Output:\")\n",
    "print(f\"  eprime_last shape: {eprime_last.shape}\")\n",
    "print(f\"  eprime_next shape: {eprime_next.shape}\")\n",
    "if logits_pos is not None:\n",
    "    pred_tail_idx = torch.argmax(logits_pos, dim=-1).item()\n",
    "    pred_tail = idx2tail[pred_tail_idx]\n",
    "    print(f\"  Predicted tail from encoder: {pred_tail}\")\n",
    "else:\n",
    "    print(\"  No predicted tail (Bpos not in lookup)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4Ô∏è‚É£ Forward pass through Inverse Decoder\n",
    "# -----------------------------\n",
    "head_emb = embed_tables['headline'].get(Bpos, torch.zeros(hidden_dim, device=device))\n",
    "c_prime, s_hat = inverse_decoder(eprime_next, behavior_encoder.b_emb, head_emb)\n",
    "\n",
    "print(\"\\n‚úÖ Inverse Decoder Output:\")\n",
    "print(f\"  c'_pos (first 6 dims): {c_prime.detach().cpu().numpy()[:6]}\")\n",
    "print(f\"  s_hat_pos (first 6 dims): {s_hat.detach().cpu().numpy()[:6]}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5Ô∏è‚É£ Forward pass through Personalized T5\n",
    "# -----------------------------\n",
    "tail_id = lookup_df.loc[Bpos]['Tail']\n",
    "print(\"Target summary id\",tail_id)\n",
    "gold_summary_text = sid2sum.get(tail_id, \"\")\n",
    "\n",
    "total_loss, logits = personalized_model(\n",
    "    Bhist, Bpos, lookup_df, tail2idx, embed_tables, sid2sum, tokenizer, max_len=50\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6Ô∏è‚É£ Decode predicted tokens\n",
    "# -----------------------------\n",
    "pred_summary = tokenizer.decode(torch.argmax(logits, dim=-1)[0], skip_special_tokens=True)\n",
    "print(\"\\n‚úÖ Personalized T5 Output:\")\n",
    "print(f\"  Target summary: {gold_summary_text[:300]}\")\n",
    "print(f\"  Predicted summary: {pred_summary[:300]}\")\n",
    "print(f\"  Total loss: {total_loss.item():.4f}\")\n",
    "print(f\"  Encoder loss: {enc_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b453884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sid2sum.get(\"S-29150\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ea732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sid2sum.get(\"S-204452\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ba9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "\n",
    "# -----------------------------\n",
    "# Training setup\n",
    "# -----------------------------\n",
    "num_epochs = 3                # adjust as needed\n",
    "batch_size = 1                # row-by-row training\n",
    "learning_rate = 3e-4          # medium LR\n",
    "save_path = \"walk2pers_checkpoint.pt\"\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    list(behavior_encoder.parameters()) +\n",
    "    list(inverse_decoder.parameters()) +\n",
    "    list(personalized_model.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "# make sure models are in train mode\n",
    "behavior_encoder.train()\n",
    "inverse_decoder.train()\n",
    "personalized_model.train()\n",
    "\n",
    "# pick first 100 rows\n",
    "train_subset = train_df.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Loop\n",
    "# -----------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    print(f\"\\nüîÑ Epoch {epoch+1}/{num_epochs} ---------------------------\")\n",
    "\n",
    "    for i, row in tqdm(train_subset.iterrows(), total=len(train_subset), desc=f\"Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Bhist = literal_eval(row['EHist'])\n",
    "        Bpos = row['EPos']\n",
    "\n",
    "        try:\n",
    "            # forward pass through full model\n",
    "            total_loss, logits = personalized_model(\n",
    "                Bhist, Bpos, lookup_df, tail2idx, embed_tables, sid2sum, tokenizer, max_len=50\n",
    "            )\n",
    "\n",
    "            # total loss weighting\n",
    "            final_loss = 0.5 * total_loss   # already includes enc_loss + gen_loss\n",
    "            # because total_loss was enc_loss + gen_loss\n",
    "            # -> if you want strict 0.5*enc + 0.5*gen, separate them here\n",
    "\n",
    "            final_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += final_loss.item()\n",
    "\n",
    "            tqdm.write(f\"[Row {i}] Loss: {final_loss.item():.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"[Row {i}] ‚ö†Ô∏è Skipped due to error: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_subset)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # save model checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch+1,\n",
    "        'behavior_encoder_state': behavior_encoder.state_dict(),\n",
    "        'inverse_decoder_state': inverse_decoder.state_dict(),\n",
    "        'personalized_model_state': personalized_model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'avg_loss': avg_loss,\n",
    "    }, save_path)\n",
    "\n",
    "    print(f\"üíæ Saved checkpoint to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2c7115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12328\\176273981.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:31<00:00, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      UserID Query(Bpos.head)  \\\n",
      "0  U100567_5          N111992   \n",
      "1  U100567_6          N123678   \n",
      "2  U100573_1           N91754   \n",
      "\n",
      "                                     Gold(Bpos.tail)  \\\n",
      "0  Jesuit school tweets #BeBrave when announcing ...   \n",
      "1  \"At 11 am we turned the pumps back on and we a...   \n",
      "2  The sportswoman, 37, was one of the pals invol...   \n",
      "\n",
      "                                    PredictedSummary         Loss      EncLoss  \n",
      "0  s iss thatFtin. theyhe  the are be be. people ...  1337.984741  1331.657104  \n",
      "1  Is the , are to corner on to and  were a the p...  1455.132690  1451.431519  \n",
      "2  industry said who, said  of the firsts who in ...   197.030609   193.422974  \n",
      "\n",
      "Saved to inference_rows_101_110.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# -----------------------------\n",
    "# 0Ô∏è‚É£ Config & load checkpoint\n",
    "# -----------------------------\n",
    "hidden_dim = 768\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ckpt_path = \"walk2pers_checkpoint.pt\"\n",
    "\n",
    "# # fresh T5\n",
    "# t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "# for p in t5_model.parameters(): p.requires_grad = False\n",
    "\n",
    "# re-instantiate wrappers\n",
    "behavior_encoder = BehaviorEncoder(hidden_dim, tail2idx, device).to(device)\n",
    "inverse_decoder = BehaviorInverseDecoderPredict(hidden_dim, device).to(device)\n",
    "personalized_model = PersonalizedT5Summarizer(hidden_dim, summarizer_model, behavior_encoder, inverse_decoder, device).to(device)\n",
    "\n",
    "# load checkpoint weights\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "behavior_encoder.load_state_dict(ckpt[\"behavior_encoder_state\"], strict=False)\n",
    "inverse_decoder.load_state_dict(ckpt[\"inverse_decoder_state\"], strict=False)\n",
    "personalized_model.load_state_dict(ckpt[\"personalized_model_state\"], strict=False)\n",
    "\n",
    "behavior_encoder.eval(); inverse_decoder.eval(); personalized_model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# 1Ô∏è‚É£ Slice rows 101‚Äì110\n",
    "# -----------------------------\n",
    "subset = train_df.iloc[200:210].reset_index(drop=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "# -----------------------------\n",
    "# 2Ô∏è‚É£ Loop with tqdm\n",
    "# -----------------------------\n",
    "for _, row in tqdm(subset.iterrows(), total=len(subset)):\n",
    "    try:\n",
    "        Bhist = literal_eval(row['EHist'])\n",
    "    except Exception:\n",
    "        Bhist = row['EHist']\n",
    "    Bpos  = row['EPos']\n",
    "    user  = row.get('UserID', None)\n",
    "\n",
    "    # Behavior Encoder\n",
    "    eprime_last, eprime_next, logits_pos, enc_loss = behavior_encoder(\n",
    "        Bhist, Bpos, lookup_df, tail2idx, embed_tables\n",
    "    )\n",
    "\n",
    "    # Inverse Decoder\n",
    "    head_emb = embed_tables['headline'].get(Bpos, torch.zeros(hidden_dim, device=device))\n",
    "    c_prime, s_hat = inverse_decoder(eprime_next, behavior_encoder.b_emb, head_emb)\n",
    "\n",
    "    # Gold summary (Bpos.tail)\n",
    "    tail_id = lookup_df.loc[Bpos]['Tail']\n",
    "    gold_summary_text = sid2sum.get(tail_id, \"\")\n",
    "\n",
    "    # Personalized T5 forward\n",
    "    total_loss, logits = personalized_model(\n",
    "        Bhist, Bpos, lookup_df, tail2idx, embed_tables, sid2sum, tokenizer, max_len=60\n",
    "    )\n",
    "\n",
    "    # Decode predicted tokens\n",
    "    pred_summary = tokenizer.decode(torch.argmax(logits, dim=-1)[0], skip_special_tokens=True)\n",
    "\n",
    "    # Query (Bpos.head body text)\n",
    "    head_id = lookup_df.loc[Bpos]['Head']\n",
    "    query_text = nid2body.get(head_id, \"\")\n",
    "\n",
    "    results.append({\n",
    "        \"UserID\": user,\n",
    "        \"Query(Bpos.head)\": head_id,\n",
    "        \"Gold(Bpos.tail)\": gold_summary_text,\n",
    "        \"PredictedSummary\": pred_summary,\n",
    "        \"Loss\": total_loss.item(),\n",
    "        \"EncLoss\": enc_loss.item() if enc_loss is not None else None\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# 3Ô∏è‚É£ Save + show results\n",
    "# -----------------------------\n",
    "df_out = pd.DataFrame(results)\n",
    "df_out.to_csv(\"inference_rows_101_110.csv\", index=False)\n",
    "\n",
    "print(df_out.head(3))\n",
    "print(\"\\nSaved to inference_rows_101_110.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2701689f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Query(Bpos.head)</th>\n",
       "      <th>Gold(Bpos.tail)</th>\n",
       "      <th>PredictedSummary</th>\n",
       "      <th>Loss</th>\n",
       "      <th>EncLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U100567_5</td>\n",
       "      <td>N111992</td>\n",
       "      <td>Jesuit school tweets #BeBrave when announcing ...</td>\n",
       "      <td>s iss thatFtin. theyhe  the are be be. people ...</td>\n",
       "      <td>1337.984741</td>\n",
       "      <td>1331.657104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U100567_6</td>\n",
       "      <td>N123678</td>\n",
       "      <td>\"At 11 am we turned the pumps back on and we a...</td>\n",
       "      <td>Is the , are to corner on to and  were a the p...</td>\n",
       "      <td>1455.132690</td>\n",
       "      <td>1451.431519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U100573_1</td>\n",
       "      <td>N91754</td>\n",
       "      <td>The sportswoman, 37, was one of the pals invol...</td>\n",
       "      <td>industry said who, said  of the firsts who in ...</td>\n",
       "      <td>197.030609</td>\n",
       "      <td>193.422974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U100573_2</td>\n",
       "      <td>N67596</td>\n",
       "      <td>Police weren't looking for anyone else in conn...</td>\n",
       "      <td>are't  for the to to the with the matter. but ...</td>\n",
       "      <td>1390.650269</td>\n",
       "      <td>1387.442627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U100573_3</td>\n",
       "      <td>N83990</td>\n",
       "      <td>Many say their constituents have expressed lit...</td>\n",
       "      <td>of that owns are been their concern in thereac...</td>\n",
       "      <td>1570.151367</td>\n",
       "      <td>1565.858887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>U100592_1</td>\n",
       "      <td>N115478</td>\n",
       "      <td>\"But Larry Lemaster would never want one perso...</td>\n",
       "      <td>I  isa is not have to to to be theira single m...</td>\n",
       "      <td>421.548828</td>\n",
       "      <td>417.467224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>U100592_2</td>\n",
       "      <td>N85945</td>\n",
       "      <td>Related slideshow: Celebrity weddings of 2019</td>\n",
       "      <td>to showsating ands are the and</td>\n",
       "      <td>699.723999</td>\n",
       "      <td>693.255920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>U100592_3</td>\n",
       "      <td>N104663</td>\n",
       "      <td>Great food is a given, reviewers say, but what...</td>\n",
       "      <td>Britain and a major. buter said. but they they...</td>\n",
       "      <td>1049.669678</td>\n",
       "      <td>1045.056885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>U100592_4</td>\n",
       "      <td>N23797</td>\n",
       "      <td>He would soon drop \"The,\" branch the network o...</td>\n",
       "      <td>said have be histhe   of company. of the Unite...</td>\n",
       "      <td>1152.062988</td>\n",
       "      <td>1146.229980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>U100602_1</td>\n",
       "      <td>N96591</td>\n",
       "      <td>It's no surprise that a lot of those concerns ...</td>\n",
       "      <td>iss  secret that thea few of people who are be...</td>\n",
       "      <td>129.862152</td>\n",
       "      <td>125.625076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserID Query(Bpos.head)  \\\n",
       "0  U100567_5          N111992   \n",
       "1  U100567_6          N123678   \n",
       "2  U100573_1           N91754   \n",
       "3  U100573_2           N67596   \n",
       "4  U100573_3           N83990   \n",
       "5  U100592_1          N115478   \n",
       "6  U100592_2           N85945   \n",
       "7  U100592_3          N104663   \n",
       "8  U100592_4           N23797   \n",
       "9  U100602_1           N96591   \n",
       "\n",
       "                                     Gold(Bpos.tail)  \\\n",
       "0  Jesuit school tweets #BeBrave when announcing ...   \n",
       "1  \"At 11 am we turned the pumps back on and we a...   \n",
       "2  The sportswoman, 37, was one of the pals invol...   \n",
       "3  Police weren't looking for anyone else in conn...   \n",
       "4  Many say their constituents have expressed lit...   \n",
       "5  \"But Larry Lemaster would never want one perso...   \n",
       "6      Related slideshow: Celebrity weddings of 2019   \n",
       "7  Great food is a given, reviewers say, but what...   \n",
       "8  He would soon drop \"The,\" branch the network o...   \n",
       "9  It's no surprise that a lot of those concerns ...   \n",
       "\n",
       "                                    PredictedSummary         Loss      EncLoss  \n",
       "0  s iss thatFtin. theyhe  the are be be. people ...  1337.984741  1331.657104  \n",
       "1  Is the , are to corner on to and  were a the p...  1455.132690  1451.431519  \n",
       "2  industry said who, said  of the firsts who in ...   197.030609   193.422974  \n",
       "3  are't  for the to to the with the matter. but ...  1390.650269  1387.442627  \n",
       "4  of that owns are been their concern in thereac...  1570.151367  1565.858887  \n",
       "5  I  isa is not have to to to be theira single m...   421.548828   417.467224  \n",
       "6                     to showsating ands are the and   699.723999   693.255920  \n",
       "7  Britain and a major. buter said. but they they...  1049.669678  1045.056885  \n",
       "8  said have be histhe   of company. of the Unite...  1152.062988  1146.229980  \n",
       "9  iss  secret that thea few of people who are be...   129.862152   125.625076  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
